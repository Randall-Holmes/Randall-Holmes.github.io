\documentclass[12pt]{article}

\usepackage{amssymb}

\title{Some points of logic, by Holmes}

\author{Randall Holmes}

\begin{document}

\maketitle

The book so far has avoided talking about logic too much directly, preferring to emphasize the development of mathematics from primitive notions and axioms, with the essentials of correct reasoning being understood.  Unchecked, one can spend an entire semester talking about correct reasoning in formal detail and not end up doing any correct reasoning about mathematical content...other than the logic itself, which is also mathematical content.

In these notes I will talk just a little about formal notions of mathematical logic.

\section{Propositional Logic}

To begin with, we view mathematical propositions as either true or false in every case, though we may not always know what the truth value is.  The basic operations of propositional logic depend only on the truth values of the sentences they connect (which is not always true of the English words and phrases we use to express them) which makes truth tables appropriate for their definitions.

In this section on propositional logic, we can represent sentences as single letters $P,Q,R\ldots$:  this is an unusual use of variables but you have probably seen it before.

I present a series of basic operations.  For each one, I give a definition, discuss its relation to the corresponding English words or phrases, talk about how to approach proving a proposition of that form, and how to approach using a proposition of that form which you are assuming or have proved in an argument.

\subsection{Not (negation) -- very brief introduction}

We use $\neg P$ to say that $P$ is false, or equivalently, ``It is not the case that $P$".

$$\begin{array}{c|c}

P & \neg P \\ \hline
T & F \\
F & T \\
\end{array}$$

We mention negation briefly because we use it in the discussion of methods of reasoning with other operations before we really want to discuss its own special rules.

\subsection{And (conjunction)}

We use $P \wedge Q$ to translate ``$P \wedge Q$, and it is a pretty good translation of a particular use of English ``and", namely, when this word is used to connect sentences.

We succinctly define it using a truth table:

$$\begin{array}{cc|c}
P & Q & P \wedge Q \\ \hline
T & T & T \\
T & F & F \\
F & T & F \\
F & F & F \\
\end{array}$$

The uses in ``John and Mary love ice cream" and ``John plays tennis and badminton" can be expressed in terms of $\wedge$ but are not literally uses of $\wedge$, since this operation (which we call ``conjunction") connects sentences, not noun or verb phrases.  These sentences expand into ``John loves ice cream and Mary loves ice cream" and ``John plays tennis and John plays badminton".  This sort of use of the propositional operations to connect noun or verb phrases certainly does occur in mathematical English, but traditionally we do not discuss this usage in formal logic (though we could):  we suppose that such sentences are expanded as indicated.

The uses in ``John and Mary carried the half ton safe" and ``14 and 37 are relatively prime" are not uses of $\wedge$ at all.
\newpage
To prove a conjunction proceed as follows:

\begin{description}

\item[Goal: ] Prove $P \wedge Q$.

\begin{description}

\item[subgoal 1:]  Prove $P$

$\vdots$

\item[14:] $P$

\item[subgoal 2:] Prove $Q$

$\vdots$

\item[37:] $Q$

\item[38:]  $P \wedge Q$ rule of conjunction, 14,37

\end{description}

The line numbers are quite arbitrary.  This proof strategy is supported by having the rule of conjunction,

$$\begin{array}{c}

P \\

Q \\ \hline

P \wedge Q \\
\end{array}$$


\end{description}

Suppose you have proved or allowed yourself to assume $P \wedge Q$.  Then you can at any time pull out $P$ or $Q$ as further things you are entitled to assume.

This is the rule of simplification, which comes in two flavors,

$$\begin{array}{c}

P \wedge Q \\ \hline

P  \\
\end{array}$$

and 

$$\begin{array}{c}

P \wedge Q \\ \hline

Q  \\
\end{array}$$

I'm not really teaching formal line by line proofs here;  what you should notice is that these formal line-by-line style rules correspond to how we would reason about an ``and" sentence in English, quite precisely.

\newpage

\subsection{If, then (implication or conditional):}

The mathematical definition of ``if$\dots$, then $\ldots$" may not look like your informal understanding of this phrase.
It supports reasoning rules very much like those of the English phrase, but its definition is definitely surprising.

We write ``If $P$, then $Q$" as $P \rightarrow Q$, and we define it thus:

$$\begin{array}{cc|c}
P & Q & P \rightarrow Q \\ \hline
T & T & T \\
T & F & F \\
F & T & T \\
F & F & T \\
\end{array}$$

We stipulate that whenever we state an implication in mathematics, this is what we mean.

Notice that a false statement implies any other statement, and any statement implies a true statement.  To see how this disagrees with English usage:  we are not entirely comfortable with the statement ``If Napoleon conquered China, then 2+2=5".  Under the definition given, this sentence is simply true.  A brief way of expressing the discomfort we feel is that the folk notion of implication probably assumes some relation of causality or at least relevance between the two statements connected.

There are a lot of equivalent ways to state $P \rightarrow Q$.  If $P$, then $Q$, $Q$ if $P$, $P$ only if $Q$, $P$ is sufficient for $Q$, and $Q$ is necessary for $P$ are all fairly common expressions.

To prove $P \rightarrow Q$, the direct strategy is

\begin{description}

\item[Goal:]  Prove $P \rightarrow Q$

\begin{description}
\item[Assume:] $P$

\item[Goal:] $Q$

$\vdots$

$Q$


\end{description}
\item [Thus:]  $P \rightarrow Q$
\end{description}

Notice that the indented block where we prove $Q$ should be ignored after we have finished the proof of $P \rightarrow Q$, because whatever is proved in it may have used the assumption $P$ which we made only for the sake of argument.

That this method is valid we can see from the truth table.  $P$ is either true or false.  If it is false, $P \rightarrow Q$ is true.  So it is enough to show that if we assume $P$ is true, it must follow that $Q$ is true (which makes $P \rightarrow Q$ true).  It is also simply a natural method of proof of an implication.

There is an alternative indirect method, proof of the contrapositive.

\begin{description}

\item[Goal:]  Prove $P \rightarrow Q$

\begin{description}
\item[Assume:] $\neg Q$

\item[Goal:] $\neg P$

$\vdots$

$\neg P$


\end{description}
\item [Thus:]  $P \rightarrow Q$
\end{description}

To use an implication there are two formal rules, each of which has a fancy Latin name, and each of which is pure common sense even on the folk reading of implication.

The rule of {\em modus ponens\/} says that if you have $P$, and you have ``if $P$, then $Q$", then you have $Q$:

$$\begin{array}{c}

P \\

P \rightarrow Q \\ \hline

Q \\
\end{array}$$

The rule of {\em modus tollens\/} says that if you have $\neg Q$, and you have ``if $P$, then $Q$", then you have $\neg P$:

$$\begin{array}{c}

\neg Q \\

P \rightarrow Q \\ \hline

\neg P \\
\end{array}$$

\subsection{Or (disjunction)}

The mathematical definition of ``or" exactly captures one sense of English ``or".  $P \vee Q$ means ``$P$ or $Q$ or both", the inclusive sense of or, and we further stipulate that in mathematical English the word ``or" (not just the symbol) {\em always\/} has this meaning.  Lawyers use and/or when they want to be clear that this is what they mean.


$$\begin{array}{cc|c}
P & Q & P \vee Q \\ \hline
T & T & T \\
T & F & T \\
F & T & T \\
F & F & F \\
\end{array}$$

is the precise truth table definition.

The exclusive sense of or has its own truth table.

$$\begin{array}{cc|c}
P & Q & P \oplus Q \\ \hline
T & T & F \\
T & F & T \\
F & T & T \\
F & F & F \\
\end{array}$$

It doesn't always have the same symbol, and we will not often if ever mention it.  Computer scientists call it XOR.

We reiterate:  when we say or, we always mean $\vee$, just as, when we say if/then, we always mean $\rightarrow$.

Our favorite strategy for proving an or statement looks rather like the strategy for proving an implication.

\begin{description}

\item[Goal:]  Prove $P \vee Q$

\begin{description}
\item[Assume:] $\neg P$

\item[Goal:] $Q$

$\vdots$

$Q$


\end{description}

\item[Thus:]  $P \vee Q$

\end{description}

If $P$ is true, then of course $P \vee Q$ is true.  If we can show that in the other case where $P$ is false, we must have $Q$, then we have $P \vee Q$ iin both cases.

\newpage

This proof strategy

\begin{description}

\item[Goal:]  Prove $P \vee Q$

\begin{description}
\item[Assume:] $\neg Q$

\item[Goal:] $P$

$\vdots$

$P$


\end{description}

\item[Thus:]  $P \vee Q$

\end{description}

also works, because disjunction is commutative.  This is not a second part of the proof of $P \vee Q$:  it is another way to give a complete proof.

We give an actual example of a proof of this form, from your homework

\begin{description}

\item[Theorem:]  For any integers $a,b$, if $ab=0$ then $a=0$ or $b=0$.

\item[Proof:]

Let $a,b$ be arbitrarily chosen integers.

\begin{description}

\item[Assume:]  $ab=0$ (using the implication proof strategy)

\item[Goal:]  $a=0$ or $b=0$

\begin{description}

\item[Assume:]  $\neg a=0$, that is, $a \neq 0$ (using the disjunction strategy)

\item[Goal:]  $b=0$

\item[the main argument:]  Because $ab=0$, we have $ab = a0$ (Prop 1.14) so we have $b=0$ (multiplicative cancellation, using the assumption that $a \neq 0$)

\end{description}


\end{description}


\end{description}

To reassure you about what is expected, you should know that you can write this much more briefly.  I am being very explicit because I am carefully explaining the logical strategy.

Here is a perfectly acceptable proof:

\begin{description}

\item[Theorem:]  For any integers $a,b$, if $ab=0$ then $a=0$ or $b=0$.

\item[Proof:]  Let $a,b$ be arbitrarily chosen integers.  Suppose that $ab=0$.  Our goal is to show $a=0$ or $b=0$.
If $a=0$ we are done, so suppose $a \neq 0$ and show that $b=0$ must be true.   We have $ab=0$, so we have $ab=a0$ by prop 1.14, so we have $b=0$ by multiplicative cancellation (using the assumption $a\neq 0$).  And we are done.

\end{description}

There is a nice set of strategies for using not and or together.  The rule of {\em disjunctive syllogism\/} in one of its forms says that if we have $P \vee Q$ and we have $\neg Q$, we get $P$.  Commutativity of disjunction and double negation give us four different forms:

\begin{enumerate}
\item

$$\begin{array}{c}

P\vee Q \\

\neg Q \\ \hline

P \\
\end{array}$$

\item

$$\begin{array}{c}

P\vee Q \\

\neg P \\ \hline

Q \\
\end{array}$$

\item

$$\begin{array}{c}

P\vee \neg Q \\

Q \\ \hline

P \\
\end{array}$$

\item

$$\begin{array}{c}

\neg P\vee Q \\

P \\ \hline

Q \\
\end{array}$$

\end{enumerate}

\newpage

An important way to use an or statement which you have proved or assumed is {\em proof by cases\/}

\begin{description}

\item[Given:]  $P \vee Q$ (already proved or assumed)

\item[Goal:] $C$

\begin{description}

\item[Case 1:]  Assume $P$

$\vdots $

\item $C$

\item[Case 2:]  Assume $Q$

$\vdots$

\item $C$

\end{description}

\item[Thus:] $C$

\end{description}

This can be presented as a rule:

$$\begin{array}{c}

P \vee Q\\

P \rightarrow C\\

Q \rightarrow C \\ \hline

C \\


\end{array}$$

To see this, notice that the proof of Case 1 above is a proof of $P \rightarrow C$ and the proof of case 2 is a proof of $Q \rightarrow C$.

We will show in the next section that we can prove the validity of this rule from rules we have already given, just for fun.

\subsection{Negation:}

We have given the definition already, and we have already stated many rules involving negation.
But it has its own special strategies and rules.

It is convenient to introduce a special symbol $\perp$ which simply represents a false statement.
\newpage
We have two basic ways to use a negative statement, apart from the ones given above.

Double negation:

$$\begin{array}{c}


\neg \neg P\\ \hline

P \\


\end{array}$$

and contradiction:

$$\begin{array}{c}
P\\

\neg P\\ \hline

\perp \\


\end{array}$$

These should both be common sense.  The rule deducing $\neg\neg P$ from $P$ is also valid and useful, but it can be proved from the other rules we give.

We present two proof strategies, both of which may suggest ``proof by contradiction" or {\em reductio ad absurdum\/}, though I think only the second one is really that.  I won't object if you call the first one proof by contradiction too.

Negation introduction (direct proof of a negative statement):

\begin{description}

\item[Goal:]  Prove $\neg P$

\begin{description}
\item[Assume:] $P$

$\vdots$

\item $\perp$

\end{description}
\item[Thus:]  $\neg P$
\end{description}

Notice that this is simply a proof of $P \rightarrow \perp$.

Proof by contradiction ({\em reductio ad absurdum\/}.  Note that this is a general proof method for statements of any form at all (the last resort):

\begin{description}

\item[Goal:]  Prove $P$

\begin{description}
\item[Assume:] $\neg P$

$\vdots$

\item $\perp$

\end{description}
\item[Thus:]  $P$
\end{description}

Notice that what you are actually doing is proving $\neg\neg P$ by negation introduction and hiding an application of double negation.

There is an important thing to remember when thinking about negation in ordinary or mathematical English.  We write $\neg  P$, but the closest thing to this in English is ``It is not the case that $P$" or ``It is false that $P$", which is formal and awkward.

If $P$ is a simple sentence, we usually negate the verb:  we say ``Roses aren't red", not ``It is not the case that roses are red".  We do the same thing in math:  $a \neq b$ and $a \not\in b$ instead of $\neg a=b$ and $\neg a \in b$.

If $P$ is a complex sentence we usually apply logical transformations to move the negation to individual sentences.
Instead of saying ``It is not the case that roses are red and violets are blue", we say ``Roses are not red {\bf or} violets are not blue".

In symbols, here are the transformations:

$\neg(A \wedge B)$ is equivalent to $\neg A \vee \neg B$;  $\neg(A \vee B)$ is equivalent to $\neg A \wedge \neg B$;  $\neg(P \rightarrow Q)$ is equivalent to $P \wedge \neg Q$ (this last may be surprising because of the unusual meaning of implication in logic).  The first two transformations have a name:  they are called de Morgan's laws.

\section{Quantifiers}

This section isn't finished.  In fact, there is some stuff which belongs in the first section which I will revisit in the next lecture, and which I list here:

\begin{enumerate}

\item converse, inverse, contrapositive

\item the biconditional operation $\leftrightarrow$, if and only if (iff)

\end{enumerate}

Let $P(x), Q(x), R(x)\ldots$ represent sentences which probably have the variable $x$ in them.

The symbol $(\forall x \in S:P(x))$  means ``For all/any/each $x$ in the set $S$, $P(x)$".  The symbol $\forall$ is called the universal quantifier.

The symbol $(\exists x \in S:P(x))$ means ``For some $x$ in the set $S$, $P(x)$" or ``There exists $x$ in the set $S$ such that $P(x)$".  The symbol $\exists$ is called the existential quantifier.

 The authors like to write $(\exists x \in S \verb| such that |P(x))$ for this.  I'm interested in why they want to do this, but I think the reasons are deep and I'll try not to share my thoughts about it with you too much.  I will sometimes write the such that (or s.t.) but I may just write a colon in both a lot of the time.

In principle we do not need to bound our quantifiers in a set, but there is a historical reason why we are inclined to do this.  I'll demonstrate with a little reasoning about the related construction of sets.  $\{x : P(x)\}$ is a name for the set of all $x$ with property $P$, we might say.

We would expect naively that for any $a$, $a \in \{x:P(x)\}$ if and only if $P(a)$.  

But this statement is false.  Define $R$ as $\{x: x \not\in x\}$.  Then by the principle in the previous paragraph,
$a \in R$ iff $a \not\in a$.  So $R \in R$ iff $R \not\in R$.  Ooooops.

In the usual systems of set theory, we fix this by restriction of the set builder notation.  We allow construction of sets $\{x \in S:P(x)\}$ and assert that for any $a$, $a \in \{x \in S:P(x)\}$ iff $a \in S \wedge P(a)$.  We allow properties to defined sets, only if these sets are carved out of previously given sets, as it were.

Now this doesn't really apply directly to quantification.  We {\em can\/} for instance say $(\forall x:x=x)$, the statement that all objects without exception are equal to themselves.  The usual system of set theory does in fact allow unrestricted quantifiers.  But it seems safer to restrict our quantifiers to a particular kind of object collected in a (possibly infinite) set $S$, and we will generally do this:  when we really have to make a universal statement about absolutely everything, we will call special attention to it.

Then I spent some time talking about order of quantifiers.  For purposes of these notes, I'll give just one math example.

\begin{enumerate}

\item $(\forall x \in {\mathbb Z}:(\exists y \in {\mathbb Z}:x+y=0))$

\item $(\exists y \in {\mathbb Z}:(\forall x \in {\mathbb Z}:x+y=0))$

\end{enumerate}

These are two statements which differ only in the order of the leading quantifiers.

The first one is familiar.  It says that for each $x$, there is a $y$ such that $x+y=0$.  The choice of the $y$ depends on $x$:  of course the $y$ that works is the additive inverse $-x$.

The second one is a shocking false assertion.  It says that there is an integer $y$ such that for any integer $x$ at all, $x+y=0$.

I gave a similar analysis of the English sentences

\begin{enumerate}

\item Everyone loves someone

\item Someone is loved by everyone

\end{enumerate}

The usual convention (it isn't invariable in English, but we {\bf mandate} it in mathematical English) is that the order in which we supply quantifiers in sentences like these is dictated by the order in which the relevant noun phrases appear in the sentence.

Use $x L y$ for $x$ loves $y$.  Notice that it also means $y$ is loved by $x$ (but this rule only applies when $x$ and $y$ are really names for particular objects).

Everyone loves someone becomes Every human $x$ loves some human $y$ becomes $(\forall x \in H:(\exists y \in H:x L y))$.

Someone is loved by everyone becomes Some human $y$ is loved by every human $x$ becomes $(\exists y \in H:(\forall x \in H:x L y))$.

The second statement here says something much stronger than the first.

More discussion of quantifiers to come.

\end{document}

