\documentclass[12pt]{article}

\usepackage{amssymb}

\usepackage{comment}
\title{Class notes for Math 287 Spring 2022}

\author{Dr Holmes}



\begin{document}

\maketitle

I'm pulling together the files of class notes that exist.  I note that I perhaps should add more discussion of logic.

I am making this file available to my Spring 2023 class:  be aware that I am not always taking exactly the same approach that I took with the earlier class.

And, it is available to my Fall 2024 class and I am adding additional material as needed.

\tableofcontents

\newpage

The book so far has avoided talking about logic too much directly, preferring to emphasize the development of mathematics from primitive notions and axioms, with the essentials of correct reasoning being understood.  Unchecked, one can spend an entire semester talking about correct reasoning in formal detail and not end up doing any correct reasoning about mathematical content...other than the logic itself, which is also mathematical content.

In these notes I will talk just a little about formal notions of mathematical logic.

\section{Propositional Logic}

To begin with, we view mathematical propositions as either true or false in every case, though we may not always know what the truth value is.  The basic operations of propositional logic depend only on the truth values of the sentences they connect (which is not always true of the English words and phrases we use to express them) which makes truth tables appropriate for their definitions.

In this section on propositional logic, we can represent sentences as single letters $P,Q,R\ldots$:  this is an unusual use of variables but you have probably seen it before.

I present a series of basic operations.  For each one, I give a definition, discuss its relation to the corresponding English words or phrases, talk about how to approach proving a proposition of that form, and how to approach using a proposition of that form which you are assuming or have proved in an argument.

\subsection{Not (negation) -- very brief introduction}

We use $\neg P$ to say that $P$ is false, or equivalently, ``It is not the case that $P$".

$$\begin{array}{c|c}

P & \neg P \\ \hline
T & F \\
F & T \\
\end{array}$$

We mention negation briefly because we use it in the discussion of methods of reasoning with other operations before we really want to discuss its own special rules.

\subsection{And (conjunction)}

We use $P \wedge Q$ to translate ``$P$ and $Q$", and it is a pretty good translation of a particular use of English ``and", namely, when this word is used to connect sentences.

We succinctly define it using a truth table:

$$\begin{array}{cc|c}
P & Q & P \wedge Q \\ \hline
T & T & T \\
T & F & F \\
F & T & F \\
F & F & F \\
\end{array}$$

The uses in ``John and Mary love ice cream" and ``John plays tennis and badminton" can be expressed in terms of $\wedge$ but are not literally uses of $\wedge$, since this operation (which we call ``conjunction") connects sentences, not noun or verb phrases.  These sentences expand into ``John loves ice cream and Mary loves ice cream" and ``John plays tennis and John plays badminton".  This sort of use of the propositional operations to connect noun or verb phrases certainly does occur in mathematical English, but traditionally we do not discuss this usage in formal logic (though we could):  we suppose that such sentences are expanded as indicated.

The uses in ``John and Mary carried the half ton safe" and ``14 and 37 are relatively prime" are not uses of $\wedge$ at all.
\newpage
To prove a conjunction proceed as follows:

\begin{description}

\item[Goal: ] Prove $P \wedge Q$.

\begin{description}

\item[subgoal 1:]  Prove $P$

$\vdots$

\item[14:] $P$

\item[subgoal 2:] Prove $Q$

$\vdots$

\item[37:] $Q$

\item[38:]  $P \wedge Q$ rule of conjunction, 14,37

\end{description}

The line numbers are quite arbitrary.  This proof strategy is supported by having the rule of conjunction,

$$\begin{array}{c}

P \\

Q \\ \hline

P \wedge Q \\
\end{array}$$


\end{description}

Suppose you have proved or allowed yourself to assume $P \wedge Q$.  Then you can at any time pull out $P$ or $Q$ as further things you are entitled to assume.

This is the rule of simplification, which comes in two flavors,

$$\begin{array}{c}

P \wedge Q \\ \hline

P  \\
\end{array}$$

and 

$$\begin{array}{c}

P \wedge Q \\ \hline

Q  \\
\end{array}$$

I'm not really teaching formal line by line proofs here;  what you should notice is that these formal line-by-line style rules correspond to how we would reason about an ``and" sentence in English, quite precisely.

\newpage

\subsection{If, then (implication or conditional):}

The mathematical definition of ``if$\dots$, then $\ldots$" may not look like your informal understanding of this phrase.
It supports reasoning rules very much like those of the English phrase, but its definition is definitely surprising.

We write ``If $P$, then $Q$" as $P \rightarrow Q$, and we define it thus:

$$\begin{array}{cc|c}
P & Q & P \rightarrow Q \\ \hline
T & T & T \\
T & F & F \\
F & T & T \\
F & F & T \\
\end{array}$$

We stipulate that whenever we state an implication in mathematics, this is what we mean.

Notice that a false statement implies any other statement, and any statement implies a true statement.  To see how this disagrees with English usage:  we are not entirely comfortable with the statement ``If Napoleon conquered China, then 2+2=5".  Under the definition given, this sentence is simply true.  A brief way of expressing the discomfort we feel is that the folk notion of implication probably assumes some relation of causality or at least relevance between the two statements connected.

There are a lot of equivalent ways to state $P \rightarrow Q$.  If $P$, then $Q$, $Q$ if $P$, $P$ only if $Q$, $P$ is sufficient for $Q$, and $Q$ is necessary for $P$ are all fairly common expressions.

To prove $P \rightarrow Q$, the direct strategy is

\begin{description}

\item[Goal:]  Prove $P \rightarrow Q$

\begin{description}
\item[Assume:] $P$

\item[Goal:] $Q$

$\vdots$

$Q$


\end{description}
\item [Thus:]  $P \rightarrow Q$
\end{description}

Notice that the indented block where we prove $Q$ should be ignored after we have finished the proof of $P \rightarrow Q$, because whatever is proved in it may have used the assumption $P$ which we made only for the sake of argument.

That this method is valid we can see from the truth table.  $P$ is either true or false.  If it is false, $P \rightarrow Q$ is true.  So it is enough to show that if we assume $P$ is true, it must follow that $Q$ is true (which makes $P \rightarrow Q$ true).  It is also simply a natural method of proof of an implication.

There is an alternative indirect method, proof of the contrapositive.

\begin{description}

\item[Goal:]  Prove $P \rightarrow Q$

\begin{description}
\item[Assume:] $\neg Q$

\item[Goal:] $\neg P$

$\vdots$

$\neg P$


\end{description}
\item [Thus:]  $P \rightarrow Q$
\end{description}

To use an implication there are two formal rules, each of which has a fancy Latin name, and each of which is pure common sense even on the folk reading of implication.

The rule of {\em modus ponens\/} says that if you have $P$, and you have ``if $P$, then $Q$", then you have $Q$:

$$\begin{array}{c}

P \\

P \rightarrow Q \\ \hline

Q \\
\end{array}$$

The rule of {\em modus tollens\/} says that if you have $\neg Q$, and you have ``if $P$, then $Q$", then you have $\neg P$:

$$\begin{array}{c}

\neg Q \\

P \rightarrow Q \\ \hline

\neg P \\
\end{array}$$

\subsection{Or (disjunction)}

The mathematical definition of ``or" exactly captures one sense of English ``or".  $P \vee Q$ means ``$P$ or $Q$ or both", the inclusive sense of or, and we further stipulate that in mathematical English the word ``or" (not just the symbol) {\em always\/} has this meaning.  Lawyers use and/or when they want to be clear that this is what they mean.


$$\begin{array}{cc|c}
P & Q & P \vee Q \\ \hline
T & T & T \\
T & F & T \\
F & T & T \\
F & F & F \\
\end{array}$$

is the precise truth table definition.

The exclusive sense of or has its own truth table.

$$\begin{array}{cc|c}
P & Q & P \oplus Q \\ \hline
T & T & F \\
T & F & T \\
F & T & T \\
F & F & F \\
\end{array}$$

It doesn't always have the same symbol, and we will not often if ever mention it.  Computer scientists call it XOR.

We reiterate:  when we say or, we always mean $\vee$, just as, when we say if/then, we always mean $\rightarrow$.

Our favorite strategy for proving an or statement looks rather like the strategy for proving an implication.

\begin{description}

\item[Goal:]  Prove $P \vee Q$

\begin{description}
\item[Assume:] $\neg P$

\item[Goal:] $Q$

$\vdots$

$Q$


\end{description}

\item[Thus:]  $P \vee Q$

\end{description}

If $P$ is true, then of course $P \vee Q$ is true.  If we can show that in the other case where $P$ is false, we must have $Q$, then we have $P \vee Q$ iin both cases.

\newpage

This proof strategy

\begin{description}

\item[Goal:]  Prove $P \vee Q$

\begin{description}
\item[Assume:] $\neg Q$

\item[Goal:] $P$

$\vdots$

$P$


\end{description}

\item[Thus:]  $P \vee Q$

\end{description}

also works, because disjunction is commutative.  This is not a second part of the proof of $P \vee Q$:  it is another way to give a complete proof.

We give an actual example of a proof of this form, from your homework

\begin{description}

\item[Theorem:]  For any integers $a,b$, if $ab=0$ then $a=0$ or $b=0$.

\item[Proof:]

Let $a,b$ be arbitrarily chosen integers.

\begin{description}

\item[Assume:]  $ab=0$ (using the implication proof strategy)

\item[Goal:]  $a=0$ or $b=0$

\begin{description}

\item[Assume:]  $\neg a=0$, that is, $a \neq 0$ (using the disjunction strategy)

\item[Goal:]  $b=0$

\item[the main argument:]  Because $ab=0$, we have $ab = a0$ (Prop 1.14) so we have $b=0$ (multiplicative cancellation, using the assumption that $a \neq 0$)

\end{description}


\end{description}


\end{description}

To reassure you about what is expected, you should know that you can write this much more briefly.  I am being very explicit because I am carefully explaining the logical strategy.

Here is a perfectly acceptable proof:

\begin{description}

\item[Theorem:]  For any integers $a,b$, if $ab=0$ then $a=0$ or $b=0$.

\item[Proof:]  Let $a,b$ be arbitrarily chosen integers.  Suppose that $ab=0$.  Our goal is to show $a=0$ or $b=0$.
If $a=0$ we are done, so suppose $a \neq 0$ and show that $b=0$ must be true.   We have $ab=0$, so we have $ab=a0$ by prop 1.14, so we have $b=0$ by multiplicative cancellation (using the assumption $a\neq 0$).  And we are done.

\end{description}

There is a nice set of strategies for using not and or together.  The rule of {\em disjunctive syllogism\/} in one of its forms says that if we have $P \vee Q$ and we have $\neg Q$, we get $P$.  Commutativity of disjunction and double negation give us four different forms:

\begin{enumerate}
\item

$$\begin{array}{c}

P\vee Q \\

\neg Q \\ \hline

P \\
\end{array}$$

\item

$$\begin{array}{c}

P\vee Q \\

\neg P \\ \hline

Q \\
\end{array}$$

\item

$$\begin{array}{c}

P\vee \neg Q \\

Q \\ \hline

P \\
\end{array}$$

\item

$$\begin{array}{c}

\neg P\vee Q \\

P \\ \hline

Q \\
\end{array}$$

\end{enumerate}

\newpage

An important way to use an or statement which you have proved or assumed is {\em proof by cases\/}

\begin{description}

\item[Given:]  $P \vee Q$ (already proved or assumed)

\item[Goal:] $C$

\begin{description}

\item[Case 1:]  Assume $P$

$\vdots $

\item $C$

\item[Case 2:]  Assume $Q$

$\vdots$

\item $C$

\end{description}

\item[Thus:] $C$

\end{description}

This can be presented as a rule:

$$\begin{array}{c}

P \vee Q\\

P \rightarrow C\\

Q \rightarrow C \\ \hline

C \\


\end{array}$$

To see this, notice that the proof of Case 1 above is a proof of $P \rightarrow C$ and the proof of case 2 is a proof of $Q \rightarrow C$.

We will show in the next section that we can prove the validity of this rule from rules we have already given, just for fun.

\subsection{Negation:}

We have given the definition already, and we have already stated many rules involving negation.
But it has its own special strategies and rules.

It is convenient to introduce a special symbol $\perp$ which simply represents a false statement.
\newpage
We have two basic ways to use a negative statement, apart from the ones given above.

Double negation:

$$\begin{array}{c}


\neg \neg P\\ \hline

P \\


\end{array}$$

and contradiction:

$$\begin{array}{c}
P\\

\neg P\\ \hline

\perp \\


\end{array}$$

These should both be common sense.  The rule deducing $\neg\neg P$ from $P$ is also valid and useful, but it can be proved from the other rules we give.

We present two proof strategies, both of which may suggest ``proof by contradiction" or {\em reductio ad absurdum\/}, though I think only the second one is really that.  I won't object if you call the first one proof by contradiction too.

Negation introduction (direct proof of a negative statement):

\begin{description}

\item[Goal:]  Prove $\neg P$

\begin{description}
\item[Assume:] $P$

$\vdots$

\item $\perp$

\end{description}
\item[Thus:]  $\neg P$
\end{description}

Notice that this is simply a proof of $P \rightarrow \perp$.

Proof by contradiction ({\em reductio ad absurdum\/}.  Note that this is a general proof method for statements of any form at all (the last resort):

\begin{description}

\item[Goal:]  Prove $P$

\begin{description}
\item[Assume:] $\neg P$

$\vdots$

\item $\perp$

\end{description}
\item[Thus:]  $P$
\end{description}

Notice that what you are actually doing is proving $\neg\neg P$ by negation introduction and hiding an application of double negation.

There is an important thing to remember when thinking about negation in ordinary or mathematical English.  We write $\neg  P$, but the closest thing to this in English is ``It is not the case that $P$" or ``It is false that $P$", which is formal and awkward.

If $P$ is a simple sentence, we usually negate the verb:  we say ``Roses aren't red", not ``It is not the case that roses are red".  We do the same thing in math:  $a \neq b$ and $a \not\in b$ instead of $\neg a=b$ and $\neg a \in b$.

If $P$ is a complex sentence we usually apply logical transformations to move the negation to individual sentences.
Instead of saying ``It is not the case that roses are red and violets are blue", we say ``Roses are not red {\bf or} violets are not blue".

In symbols, here are the transformations:

$\neg(A \wedge B)$ is equivalent to $\neg A \vee \neg B$;  $\neg(A \vee B)$ is equivalent to $\neg A \wedge \neg B$;  $\neg(P \rightarrow Q)$ is equivalent to $P \wedge \neg Q$ (this last may be surprising because of the unusual meaning of implication in logic).  The first two transformations have a name:  they are called de Morgan's laws.

We prove the validity of proof by cases in rule form, just for fun, as promised.

\begin{description}


\item[Given:]  
\begin{enumerate}
\item $P \vee Q$

\item $P \rightarrow C$

\item $Q \rightarrow C$

\end{enumerate}

\item[Goal:] $C$

\begin{description}
\item[Assume (4):]  $\neg C$  for the sake of a contradiction

\item[Goal:]  $\perp$

\item[(5)] $\neg P$ 2,4, modus tollens

\item[(6)]  $Q$ 5,1 disjunctive syllogism

\item[(7)]  $C$ modus ponens 3,6

\item[(8)]  $\perp$  4,7 contradict each other

\end{description}

\item[Thus:]  $C$ by reductio ad absurdum, 4-8

\end{description}

\section{Quantifiers}

This section isn't finished.  In fact, there is some stuff which belongs in the first section which I will revisit in the next lecture, and which I list here:

\begin{enumerate}

\item converse, inverse, contrapositive

\item the biconditional operation $\leftrightarrow$, if and only if (iff)

\end{enumerate}

Let $P(x), Q(x), R(x)\ldots$ represent sentences which probably have the variable $x$ in them.

The symbol $(\forall x \in S:P(x))$  means ``For all/any/each $x$ in the set $S$, $P(x)$".  The symbol $\forall$ is called the universal quantifier.

The symbol $(\exists x \in S:P(x))$ means ``For some $x$ in the set $S$, $P(x)$" or ``There exists $x$ in the set $S$ such that $P(x)$".  The symbol $\exists$ is called the existential quantifier.

 The authors like to write $(\exists x \in S \verb| such that |P(x))$ for this.  I'm interested in why they want to do this, but I think the reasons are deep and I'll try not to share my thoughts about it with you too much.  I will sometimes write the such that (or s.t.) but I may just write a colon in both a lot of the time.

In principle we do not need to bound our quantifiers in a set, but there is a historical reason why we are inclined to do this.  I'll demonstrate with a little reasoning about the related construction of sets.  $\{x : P(x)\}$ is a name for the set of all $x$ with property $P$, we might say.

We would expect naively that for any $a$, $a \in \{x:P(x)\}$ if and only if $P(a)$.  

But this statement is false.  Define $R$ as $\{x: x \not\in x\}$.  Then by the principle in the previous paragraph,
$a \in R$ iff $a \not\in a$.  So $R \in R$ iff $R \not\in R$.  Ooooops.

In the usual systems of set theory, we fix this by restriction of the set builder notation.  We allow construction of sets $\{x \in S:P(x)\}$ and assert that for any $a$, $a \in \{x \in S:P(x)\}$ iff $a \in S \wedge P(a)$.  We allow properties to defined sets, only if these sets are carved out of previously given sets, as it were.

Now this doesn't really apply directly to quantification.  We {\em can\/} for instance say $(\forall x:x=x)$, the statement that all objects without exception are equal to themselves.  The usual system of set theory does in fact allow unrestricted quantifiers.  But it seems safer to restrict our quantifiers to a particular kind of object collected in a (possibly infinite) set $S$, and we will generally do this:  when we really have to make a universal statement about absolutely everything, we will call special attention to it.

Then I spent some time talking about order of quantifiers.  For purposes of these notes, I'll give just one math example.

\begin{enumerate}

\item $(\forall x \in {\mathbb Z}:(\exists y \in {\mathbb Z}:x+y=0))$

\item $(\exists y \in {\mathbb Z}:(\forall x \in {\mathbb Z}:x+y=0))$

\end{enumerate}

These are two statements which differ only in the order of the leading quantifiers.

The first one is familiar.  It says that for each $x$, there is a $y$ such that $x+y=0$.  The choice of the $y$ depends on $x$:  of course the $y$ that works is the additive inverse $-x$.

The second one is a shocking false assertion.  It says that there is an integer $y$ such that for any integer $x$ at all, $x+y=0$.

I gave a similar analysis of the English sentences

\begin{enumerate}

\item Everyone loves someone

\item Someone is loved by everyone

\end{enumerate}

The usual convention (it isn't invariable in English, but we {\bf mandate} it in mathematical English) is that the order in which we supply quantifiers in sentences like these is dictated by the order in which the relevant noun phrases appear in the sentence.

Use $x L y$ for $x$ loves $y$.  Notice that it also means $y$ is loved by $x$ (but this rule only applies when $x$ and $y$ are really names for particular objects).

Everyone loves someone becomes Every human $x$ loves some human $y$ becomes $(\forall x \in H:(\exists y \in H:x L y))$.

Someone is loved by everyone becomes Some human $y$ is loved by every human $x$ becomes $(\exists y \in H:(\forall x \in H:x L y))$.

The second statement here says something much stronger than the first.

More discussion of quantifiers to come.




\section{Feb 22 lecture}

I don't usually put up class notes, but there is a student out of class who requested them, and the second part of today's lecture addresses things which are not in the book.  I still owe you an extension to the previous set of notes on logic, and working with sets may induce me to get on it and produce them.

The lecture had two parts, one extending the lecture before the test about strong induction and the kind of extension to recursion which gives the Fibonacci numbers, and one an introduction to basic concepts about sets.

\section{Stuff about Strong Induction, Recursion, and Fibonacci-Like sequences, Feb 22 2022}

I'm just going to present the examples and theorems I did rather than try to say more about general principles.

\begin{description}

\item[Definition:]  Define a sequence $A$ by $A_1=2$, $A_2=5$, $A_{k+2} = 5A_{k+1}-6A_k$.

\item[Calculations:]  $A_3 = 5A_2-6A_1 = (5)(5) - (6)(2) = 13$

$A_4 = 5A_3-6A_2 = (5)(13) - (6)(5) = 35$

and so forth

\item[Theorem:]  For each natural number $n$, $A_n = 2^n+3^n$ (as is typical with induction proofs, we aren't told where this statements comes from)

\item[Proof:]  We prove this by strong induction.  

$A_1=2=1+1=2^0+3^0$, true for $n=1$ .

$A_2=5=1+1=2^1+3^1$, true for $n=2$.   (we use two pieces of information at the basis).

Let $k\geq 2$ be chosen arbitarily and assume for all $m$ with $1 \leq m \leq k$ that $A_m = 2^m+3^m$.  We already know this for $k=2$, the basis of our induction.

Our goal is to show that $A_{k+1}=2^{k+1}+3^{k+1}$.

We know by definition of the sequence $A$ that $A_{k+1} = 5A_{k}-6A_{k-1}$.  Notice that this uses our assumption
that $k \geq 2$.

Now by inductive hypothesis $5A_{k}-6A_{k-1}$ = $5(2^k+3^k)-6(2^{k-1}+3^{k-1})$.

$5(2^k+3^k)-6(2^{k-1}+3^{k-1})$ = $(10)2^{k-1}+(15)3^{k-1}-6(2^{k-1})-6(3^{k-1})$ = $4(2^{k-1})+9(3^{k-1}) = 2^{k+1} + 3^{k+1}$

And this completes the proof.

\item[Observation:]  You might ask...where does this come from?  We give a hint...suppose we had a sequence $B$ with
$B_{k+2} = 5B_{k+1}-6B_k$...and make a further guess, $B_k = r^k$ for some $r$.

$r^{k+2} = 5r^{k+1}-6r^k$ is true (if $r \neq 0$) if and only if $r^2 = 5r-6$, that is $r^2-5r+6$, which has roots 2 and 3 which you can find by standard techniques.  So the sequence of powers of 2 and the sequence of powers of 3 satisfy this recurrence relation, and it is straightforward to show that adding two sequences which have this property will give a sequence with this property.

\item[Definition:]  Define $G_k=\sum_{i=1}^k F_k$.

\item[Experiment:]  Compute the first eight terms of this sequence and look for patterns.  Two were noticed by students:
$G_{k+2}= G_k+G_{k+1}+1$, and $G_k = F_{k+2}-1$.  I admit freely that I was expecting you all to notice the second one;  the first one was a bonus.

It is surprising, perhaps that neither of these proofs needs strong induction.  In the coming homework problems involving proofs about Fibonacci numbers, be ready to use strong induction, but also be ready to find that you need nothing more than ordinary induction.

\item[Theorem:]  For all  natural numbers $n$, $G_{n+2} = G_n+G_{n+1}+1$

\item[Proof:]  For $n=1$, observe that $G_1=1, G_2=1+1=2, G_3=1+1+2=4$, and $G_3 = 4 = 1+2+1 = G_1+G_2+1$.

Now fix a natural number $k$ and assume $G_{k+2} = G_k+G_{k+1}+1$ (ind hyp).  The induction goal is
to show that $G_{k+3} = G_{k+1}+G_{k+2}+1$.

$G_{k+3} = \sum_{i=1}^{k+3}F_i = \sum_{i=1}^{k+2}F_i + F_{k+3}$ by the definition of summation
$=G_{k+2} + F_{k+3}$ by definition of $G$

$= G_k+G_{k+1}+1+F_{k+3}$ by ind hyp

$= G_k + G_{k+1}+ 1+F_{k+1}+F_{k+2}$ by definition of $F$

$= G_k+F_{k+1}  + G_{k+1}+F_{k+2}+1$ regrouping

$= \sum_{i=1}^kF_i+F_{k+1}  +\sum_{i=1}^{k+1}F_i +F_{k+2}+1$ definition of G

$= \sum_{i=1}^{k+1}F_i  +\sum_{i=1}^{k+2}F_i +1$ definition of summation

$= G_{k+1}+G_{k+2}+1$ definition of G; which is what we needed.

\item[Theorem:]  For all natural numbers $n$, $G_n=F_{n+2}-1$

\item[Proof:]  By induction.  $G_1 = 2-1 = F_3-1$, so the statement is true for $n=1$.

Fix an arbitrary natural number $k$.  Assume that $G_k = F_{k+2}-1$.   Our goal is to show $G_{k+1}=F_{k+3}-1$.

$G_{k+1} = \sum_{i=1}^{k+1}F_i$  definition of G

$= \sum_{i=1}^{k}F_i + F_{k+1}$  definition of summation

$= G_k + F_{k+1}$  definition of $G$

$= F_{k+2}-1 + F_{k+1}$  ind hyp

$ = F_{k+3}-1$ regrouping and definition of F.

\end{description}

\newpage

\section{Fall 2024 lectures of Oct 15 and 17}

I promised notes on things I did with strong induction and theorems I proved about prime numbers which are not in the Art of Proof, and here they are.

The principle of strong induction tells us that if $P[x]$ is a sentence about an integer variable $x$, we can prove
``for all $n \in \mathbb N$, $P[n]$" (in quantifier notation, $(\forall n \in \mathbb N:P[n])$) by proving (basis step) $P[1]$
and proving (strong induction step) $(\forall k \in \mathbb N:(\forall j \leq k:P[j]) \rightarrow P[k+1])$.

The strategy for proving the strong induction step is, let $k \in \mathbb N$ be chosen arbitrarily:  assume (ind hyp) that for every $j \leq k$, $P[j]$;  then the goal is to prove $P[k+1]$.   It is called {\em strong\/} induction because the inductive hypothesis is a stronger statement (contains more information).

We show that strong induction is valid using ordinary induction.

Suppose that we have (1) $P[1]$ and (2) $(\forall k \in \mathbb N:(\forall j \leq k:P[j]) \rightarrow P[k+1])$.

We have the basis $P[1]$ for a proof of $(\forall n \in \mathbb N:P[n])$ but we do not have the induction step.
because to prove $P[k+1]$ using the information given we need all of $P[0], P[1],\ldots, P[k-1],P[k]$ not just $P[k]$.

We take a tack which is not unusual in sophisticated induction proofs:  we formulate and prove a stronger statement and the stronger form of the induction hypothesis lets the proof go through.

We use the same information, $P[1]$ and $$(\forall k \in \mathbb N:(\forall j \leq k:P[j]) \rightarrow P[k+1]),$$ to prove
by ordinary induction that $Q[n]$, defined as $(\forall m \leq n:P[m])$, holds for every $n$.

$Q[1]$ is $(\forall m \leq 1:P[m])$, which is true because the only $m \leq 1$ is 1 and $P[1]$ is true by (1).

Let $k \in \mathbb N$ be chosen arbitrarily and assume $Q[k]$, that is, $(\forall m \leq k:P[m])$, which is equivalent
to $(\forall j \leq k:P[j])$ (just by changing the dummy variable) which implies $P[k+1]$ by our hypothesis (2).  Now the only $m \leq k+1$ are the $m \leq k$ and $k+1$ itself, so we have shown $(\forall m \leq k+1:P[m])$, that is $Q[k+1]$, by ordinary induction.

Now if $(\forall n \in \mathbb N:(\forall m \leq n:P[m]))$ we clearly have $(\forall n \in \mathbb N:P[n])$.

So we have justified strong induction using ordinary induction.

My favorite example of strong induction is not in the book (or at least not at this point).  So I exhibit the development here, and I prove various ``obvious" components of the result here by extreme care with product notation analogous to the extreme care with summation notation that we have been practicing.

\begin{description}

\item[Definition:]  A {\em prime number\/} is an integer $p>1$ (and so certainly a natural number) with the property
that for all $m>1$,  if $m|n$ then either $m=1$ or $m=n$.

\item[Definition:]  An integer $n$ is a {\em finite product of primes\/} iff there is a natural number $k$ and a sequence $p_i$ such that for every $i$, $p_i$ is a prime number (we say that $p_i$ is a sequence of primes) and $\prod_{i=1}^k p_i = n$.

\item[Theorem:]  Every integer $n \geq 2$ is a finite product of primes (the way we phrase this depends on the fact that by our definition, a prime $p$ is itself a finite product of primes $\prod_{i=1}^1 p$:  the sequence used may be taken to be the constant sequence all of whose values are $p$).

\item[Proof:]  We prove this by strong induction, adapted to start at 2 rather than 1.

The basis step is to prove that 2 is a finite product of primes.  Define $q_i$ as 2 for every $i$.  $2 = q_1 = \prod_{i=1}^1 q_1$

Let $k \geq 2$ be chosen arbitrarily.  Assume (inductive hypothesis) that for all $m \leq k$, $m$ is a finite product of primes.

Our goal is to show that $k+1$ is a finite product of primes.

There are two cases.  Case 1:  if $k+1$ is prime, it is trivially a finite product of primes (we have already seen how to show this).

Case 2:  suppose $k+1$ is not prime.  Then there is a natural number $m$ other than 1 or $k+1$ such that $m|k+1$.
Thus there is an integer $n$ such that $mn=k+1$.  Clearly $n$ is positive, since $m$ and $k+1$ are positive.
$n \neq 1$ since $m1 = m \neq k+1$.  Thus $m<k+1$, because if $m>k+1$ we would certainly have $k+1 = mn >k+1$ which is absurd.  Similarly $n<k+1$.

This means by inductive hypothesis that $m$ and $n$ are finite products of primes.  Common sense suggests that
the product of two finite products of primes must be a finite product of primes, but we think that in the style of chapter 4 this requires a proof.

\begin{description}

\item[Lemma:]  If $m$ and $n$ are finite products of primes, $mn$ is a finite product of primes.

This can be rephrased as, for any sequences $\{p_i\}$ and $\{q_i\}$ of primes and natural numbers $a,b$,
$(\prod _{i=1}^a p_i)(\prod_{i=1}^b q_i)$ is a product of primes.  We prove this by induction on $b$.

basis:  $(\prod _{i=1}^a p_i)(\prod_{i=1}^1 q_i) = (\prod _{i=1}^a p_i)q_1 = (\prod _{i=1}^a p_i)p'_1= (\prod _{i=1}^{a+1} p'_i)$, where
$p'_i$ is defined as $p_i$ if $i\leq a$ and $q_1$ if $i \geq a+1$, and $\prod _{i=1}^{a+1} p'_i$ is by definition a finite product of primes.

\end{description}

Using the lemma, we see that $k+1 = mn$ is a finite product of primes, and the proof of the Theorem by strong induction is complete.

\item[Remark:]  Notice that we have proved that every natural number $n \geq 2$ is a finite product of primes. but we have not proved that each natural number factors into primes in only one way.  We may look later at how to say this precisely and how to prove it.  For any $n \geq 2$, $n = \prod_{i=1}^k p_i$ for some sequence of primes $\{p_i\}$ and some natural number $k$.   We claim that $p_k | \prod_{i=1}^k p_i$, so $p_k |n$, so $n$ has a prime factor.  If $k=1$, then $p_k | \prod_{i=1}^1 p_i$ because $p_k\cdot 1 = p _1 = \prod_{i=1}^1 p_i$.  If $k>1$ then $p_k \cdot \prod_{i=1}^{k-1}p_i = \prod_{i=1}^k p_i$, so again $p_k | \prod_{i=1}^k p_i$.

\item[There are infinitely many prime numbers (Euclid):]

Suppose that there are only finitely many natural numbers, so there is a finite sequence $\{p_i\}_{1 \leq i \leq k}$ such that every prime $q$ is equal to some $p_i$.

Define $P$ as $\prod_{i=1}^k p_i$.

$P+1 \geq 2$ is certainly true, so $P+1$ has a prime factor $q$ as we showed above.  $q=p_j$ for some natural number $j$ between 1 and $k$ inclusive by choice of the sequence $\{p_i\}$.

``Obviously" $q|P$, so $q|(P+1)-P$, so $q|1$.  But this is absurd, because for any integer $r$ such that $qr>0$ (as would be the case if $qr=1$), we have $r>0$, so $r \geq 1$, so $qr \geq q>1$, so $q|1$ is impossible, since it requires $qr=1$ for some integer.

What we think requires another insanely careful Lemma is our claim that $q|P$, which is a consequence of this

\begin{description}

\item[Lemma:]  For any product of integers $\prod_{i=1}^nx_i$, $x_j | \prod_{i=1}^nx_i$ for each $j$ with $1 \leq j \leq n$.

\item[Proof:]  We prove this by induction on $k$.

basis ($n=1$)  The only $j$ with $1 \leq j \leq 1$ is 1, and $x_1 | \prod_{i=1}^1 x_i$ is obvious.

induction step:  Fix $k$ a natural number, arbitrarily.  Assume that $x_j | \prod_{i=1}^kx_i$ for each $j$ with $1 \leq j \leq k$.

Our goal is to show that $x_j | \prod_{i=1}^{k+1}x_i$ for each $j$ with $1 \leq j \leq k+1$

Either $j=k+1$ or $1 \leq j \leq k$.

If $j=k+1$, notice that $\prod_{i=1}^{k+1}x_i = (\prod_{i=1}^kx_i)x_{k+1}$, which is divisible by $x_{k+1} = x_j$.

If $1 \leq j \leq k$, we know that $x_j$ goes into $\prod_{i=1}^kx_i$, so $\prod_{i=1}^kx_i=x_jz$ for some integer $z$,
so $\prod_{i=1}^{k+1}x_i = (\prod_{i=1}^kx_i)x_{k+1}= (x_jz)x_{k+1} = x_j(zx_{k+1})$, which is evidently divisible by $x_j$.

\end{description}

The Lemma verifies that we have completed the proof that there are infinitely many primes, since the assumption that there are finitely many primes led to a contradiction.

\end{description}

The author's favorite examples of proofs by strong induction are proofs using sequences with recursive definitions where the definition of a term $x_{n}$ depends on the definition of more than one previous term of the sequence, as in the case of the Fibonacci numbers.  I may add more about this to these notes eventually, but there is an extensive example in the Art of Proof (the proof of the validity of the closed form for the Fibonacci sequence).  The previous section of the notes contains some easier examples of proofs about the Fibonacci numbers and similar sequences; I lectured one of these on Oct 17 2024.

\section{Introducing Sets, Feb 22, 2022}

The book introduces basic concepts of sets at a level needed for success in more advanced mathematics.  You may notice that they have already been using these concepts earlier.

I will take an approach which is a bit more explicit.   Without too much logic (I hope) I am going to emulate the book's treatment of natural numbers by giving some primitive notions and axioms governing the notion of set.

Some objects in the mathematical world are sets.  This is a primitive notion.

Sets have objects as elements.  We write $a \in S$ for $a$ is an element of $S$.  The membership relation is a primitive notion.

\begin{description}

\item[Axiom of Members:]  If a membership relation $a \in S$ holds, we can deduce that $S$ is a set.  (It is equivalent to say that any object which is not a set has no elements).

\end{description}

It is common in foundations of mathematics to assume that everything is a set.  We will not make this assumption, but neither will we explicitly assume that there are non-sets.

We introduce a familiar piece of notation $\{x,y\}$:  this is the set whose only elements are $x$ and $y$, an unordered pair (if $x$ and $y$ are distinct).  We call a set $\{x,x\}$ a singleton and feel free to write $\{x\} = \{x,x\}$.

\begin{description}

\item[Axiom of Pairs:]  For any objects $x,y$ (not necessarily distinct) there is a set $\{x,y\}$.  For any $z$, $z \in \{x,y\}$ if and only if $z=x \vee z=y$.

\end{description}

This notation (which should be familiar to you) can be used to make an important point.  Whatever elements are, they are not parts of the sets they belong to.  Let $a,b$ be distinct objects and consider the set $\{\{a,b\}\}$.  This set has only one element $\{a,b\}$, so at least one of $a,b$ does not belong to it:  suppose wlog that $a \not\in \{\{a,b\}\}$.  So we have $a \in \{a,b\}$ and $\{a,b\} \in \{\{a,b\}\}$ but $a \not\in \{\{a,b\}\}$:  the membership relation is not transitive.  So members of sets are not in general parts of sets:  the relationship of part to whole is transitive.

There is another important relation between sets which is a much better candidate for the relation of part to whole between sets.

\begin{description}

\item[Definition:]  The relation $A \subseteq B$ is defined as holding if and only if $(\forall x \in A: x \in B)$:  that is, if every element of $A$ is an element of $B$.

\item[Theorem:]   For any set $A$, $A \subseteq A$.  

\item[Theorem:]   If $A \subseteq B$ and $B \subseteq C$ then $A \subseteq C$.

\item[Proof:]  Suppose that $A \subseteq B$ and $B \subseteq C$

Let $x$ be chosen arbitrarily.  Suppose $x \in A$.  Our goal is to show $x \in C$.

(can you see that this is a plan to prove the Theorem?)

Since $x \in A$ and $A \subseteq B$, it follows that $x \in B$.

Since $x \in B$ and $B \subseteq C$, it follows that $x \in C$.

So we have shown that any element of $A$ must belong to $C$, which is what it means for $A \subseteq C$ to be true.

\end{description}

The subset relation, being transitive, is a much more reasonable implementation of the idea of a {\em part\/} of a set.

This relation can be used to state the criterion for identity of sets.

\begin{description}

\item[Axiom of Extensionality:]  If $A$ and $B$ are sets, $A = B$ if and only if $A \subseteq B$ and $B\subseteq A$.  Equivalently, sets $A$ and $B$ are equal exactly if they have the same elements (every element of $A$ is an element of $B$ and every element of $B$ is an element of $A$).

\item[Example:]  We can now prove that $\{a,b\} = \{b,a\}$.

\end{description}

We introduce another interesting object.

\begin{description}

\item[Axiom of the Empty Set:]  There is a set $\emptyset$ such that $x \in \emptyset$ is false for any object $x$.

\item[Theorem:]  For any set $X$ with no elements and any set $A$, $X \subseteq A$ holds.  In particular, $\emptyset \subseteq A$.

\item[Proof:]  Suppose that $X$ is a set with no elements.  Then for any object $x$, if $x \in X$, $x \in A$, because a false statement implies anything.  So all elements of $X$ (none of them) are in $A$, so $X \subseteq A$.  $\emptyset$ has no elements, so $\emptyset \subseteq A$ by the same argument.

\item[Observation:]  This does {\bf NOT} say that the empty set belongs to every set as an element.

\item[Theorem:]  Suppose that $X$ is a set with no elements.  Then $X = \emptyset$.  There is only one empty set.

\item[Proof:]  By the previous Theorem, $X \subseteq \emptyset$ and $\emptyset \subseteq X$, so $X = \emptyset$ by the Axiom of Extensionality.

\end{description}

We have used sets already in this book, usually correlated with properties.  The principle we are using can be expressed formally:

\begin{description}

\item[Axiom of Separation:]  Let $S$ be a set and let $P(x)$ be a sentence expressing a property of $x$.  There is a set $\{x \in S:P(x)\}$ such that for every $a$, $a \in \{x \in S:P(x)\}$ if and only if $a \in S$ and $P(a)$.

\end{description}

When we use the well-ordering principle to show that all numbers have some property, we are usually applying the axiom of separation.   Suppose we are trying to prove that all numbers $x$ have some property $P(x)$.  Suppose not.  Then there is some natural number $n$ such that $\neg P(n)$, so the set $\{x \in {\mathbb N}:\neg P(x)\}$ is nonempty, so it has a smallest element (the least counterexample)...and then we reason to a contradiction.

Notice that the axiom of separation lets us define sets only if we are already given sets to carve them out of.  We give some additional axioms which provide us with grist for our mill.

\begin{description}

\item[Axiom of Power Set:]  For any set $A$, there is a set ${\cal P}(A)$, called the power set of $A$, such that $B \in {\cal P}(A)$ exactly if $B \subseteq A$, for any $B$.  ${\cal P}(A)$ can be called...the set of all subsets of $A$.

\end{description}

We look at familiar Venn diagram operations.  $A \cap B$ can be defined as $\{x \in A:x \in B\}$, which exists by the axiom of separation.  $A - B$ can be defined as $\{x \in A: x \not\in B\}$, again provided by the axiom of separation.  For unions, we need the

\begin{description}

\item[Axiom of Binary Union:]  For any sets $A,B$ there is a set $A \cup B$ such that for any $x$, $x \in A \cup B$ if and only if either $x \in A$ or $x \in B$.

\end{description}

Using the axioms of pairing and binary union, we can construct all finite sets.

\begin{description}

\item[Definition:]  We are given the notation $\{x_1,x_2\}$ for a finite set with two elements.  If we have defined
the notation $\{x_1,\ldots,x_n\}$, we define $\{x_1,\ldots,x_n,x_{n+1}\}$  as $\{x_1,\ldots,x_n\} \cup \{x_{n+1}\}$.

\end{description}

We are given some infinite sets, such as $\mathbb N$.  We can simply postulate this set and its axioms as earlier in the book.

We could also present an implementation.  We give the original approach of Zermelo.  Define 0 as $\emptyset$.
Define $n+1$ (temporarily) as $\{n\}$.

\begin{description}

\item[Axiom of Infinity:]  There is a set $\cal Z$ such that $0 \in {\cal Z}$ and for every $x$, if $x \in {\cal Z}$ then $x+1 = \{x\} \in {\cal Z}$.

\item[Definition:]  We say that a set $I$ is {\em inductive\/} iff $0 \in I$ and for every $x$, if $x \in I$ then $x+1 = \{x\} \in I$.  Notice that the axiom of infinity simply says that there is an inductive set.

\item[Definition:]  Let $\cal Z$ be an inductive set.  Define ${\cal Z}_0$  as the collection of all $n$ such that for every inductive element $I$ of ${\cal P}({\cal Z})$, $n \in I$.

\item[Theorem:]  Any element of ${\cal Z}_0$ belongs to {\em every\/} inductive set.  And any object which belongs to all inductive sets belongs to ${\cal Z}_0$. 

\item[Proof:]  Let $n \in {\cal Z}_0$.  Let $J$ be an inductive set.  Then $J \cap {\cal Z}$ is an inductive set and an element of ${\cal P}({\cal Z})$.  So $n \in J \cap {\cal Z}$.   So $n \in J$.

If $x$ belongs to every inductive set, of course it belongs to every inductive set in the power set of ${\cal Z}$, and so belongs to ${\cal Z}_0$.
 

\end{description}

The previous theorem shows that the set ${\cal Z}_0$ is the same set no matter what inductive set $\cal Z$ we start with, and so should have a name of its own.  We might suggest $\mathbb N$ as its name, if we were comfortable with the construction $0 =\emptyset; 1 = \{0\}; 2 = \{1\}; 3 = \{2\},$ and so forth (and if we included 0 in the natural numbers).

We note that nowadays there is a standard definition of the non-negative integers as sets, a somewhat different one, which works equally well and has one nice property that Zermelo's definition does not have.  Define 0 as $\emptyset$ and $n+1$ as
$n \cup \{n\}$, and state the axiom of infinity using this operation instead of the singleton operation.  This leads to
the construction $0 = \emptyset;  1 = \{0\}; 2 = \{0,1\}; 3 = \{0,1,2\},$ and so forth.  This has the nice property that the set we identify with $n$ has $n$ elements.



I'm not going to say that either of these is our official definition.  I think it is much more interesting to notice that an implementation of the system of natural numbers using sets is possible, and also that more than one such implementation is possible.  We have given only a hint of the full implementation in either case, since one would also need to define the operations of addition and multiplication (which can certainly be done).

I could specifically assert the existence of further sets such as the set of rational numbers or the set of real numbers, but it turns out that just asserting the existence of an infinite set is enough to construct sets implementing these familiar number systems and basically all mathematical structures that you will study.

The set of axioms we have given here is hardly a complete set of axioms, but it ought to support most of the work that is described in this book.  And I am again rather more interested in you being aware that axioms for the set concept can be presented than in the details.

\section{More remarks on sets, from the Feb 24 Lecture}

In this lecture, I worked directly from the set section of the Art of Proof.  I'll record some comments in these notes whcih aren't directly from their text, or which I think are particularly interesting.

I note as important a general proof strategy (really, two of them):

\begin{description}

\item[To prove $A \subseteq B$:]  If $A$ and $B$ are sets, to prove that $A$ is a subset of $B$, introduce an arbitarily chosen object $x$, assume $x \in A$, and then deduce $x \in B$.

\item[To prove equality of two sets:]  If $A$ and $B$ are sets:

\begin{description}

\item[Goal:]  $A=B$

\item[Part I:]  

\begin{description}
\item[Let:]  $x$ be arbitrarily chosen

\item[Assume:]  $x \in A$

\item[Goal:]  $x \in B$

\item[proof steps \vdots]

\item[finishing part I:]  $x \in B$

\end{description}

\item[Part II:]  

\begin{description}
\item[Let:]  $y$ be arbitrarily chosen

\item[Assume:]  $y \in B$

\item[Goal:]  $y \in A$

\item[proof steps \vdots]

\item[finishing part II:]  $y \in A$

\end{description}

\end{description}

Notice that Part I shows $A \subseteq B$ and Part II shows $B \subseteq A$.

\end{description}

I discussed set definitions with expressions to the left of the colon.  The sets we are allowed by the Axiom of Separation all have the form $\{x \in A:P(x)\}$.  How do we explain a set like $\{7m+1:m \in {\mathbb Z}\}$ in a way which makes it clear that we are allowed to assume this set?

$\{7m+1:m \in {\mathbb Z}\} = \{k \in {\mathbb Z}: (\exists m\in {\mathbb Z}: k = 7m+1)\}$ is an explanation of this notation:
it is clear that the second expression is given to us by the axiom of separation.

A general explanation where there is one variable to the left of the colon is, when $f(x)$ is an expression that we know will be in set $A$ if $x$ is in set $B$, then $\{f(x):x \in B\}$ means $\{k \in A:(\exists x\in B:k = f(x))\}$.

A more complicated example could appear in a definition of the gcd.  We could define ${\tt gcd}(a,b)$ as
$\{ax+by \in {\mathbb N}:x \in {\mathbb Z} \wedge y \in {\mathbb Z}\}$.  This would expand to $$\{k \in {\mathbb N}:(\exists x,y \in {\mathbb Z}: k = ax+by\}.$$

Notice that both variables appearing to the left of the colon in the first definition end up being existentially quantified in the body of the second set definition.  It is interesting to observe that I proposed this example then looked back to see what the authors had done in their definition...and they had given the second form!

I believe that you have all been expected in the past and will be expected in the future to read set definitions with complex terms to the left of the colon;  I think a detailed formal definition here would be threatening, but a couple of examples of how to explain these definitions, which I have given, should help you to see that this kind of set definition fits into the framework I am presenting.

I suggest reading the proof the authors give of proposition 5.2.

I discussed the difference between the two set definitions given in project 5.5 part b.  The crucial thing to recognize
is that $m$ is a dummy variable in the definition of $U$, but a fixed number given before the set is defined in the definition of $V$, and this gives quite different results for what the sets look like. 

The authors use definitions of the form $\{x:P(x)\}$ without a bounding set in defining intersection, union, and set difference.  You will see above that we avoid doing this, basically because we cannot assume for a general sentence $P(x)$ that there
even {\em is\/} a set $\{x:P(x)\}$:  we know that $\{x :x \not\in x\}$ does not exist!

What I prefer to do is to define $A \cap B$ as $\{x \in A:x \in B\}$ (or equivalently $\{x \in B:x \in A\}$) and
define $A - B$ as $\{x \in A:x \not\in B\}$, both of which set constructions are justified by the axiom of separation, and then appeal to the axiom of binary unions above, and simply assert that for any $A,B$, there is a set $A \cup B$, and for any $c$,
$c \in A \cup B$ if and only if $c \in A \vee c \in B$.  The problem with union is that for completely general sets $A,B$ we don't have an obvious way to define a bounding set $X$ such that $\{x \in X:x \in A \vee x \in B\}$ is actually $A \cup B$ without in effect assuming an axiom that for any sets $A,B$, there is a set $X$ of which both are subsets (the axiom of binary union does this, of course).

It isn't an error to use the notation $\{x:P(x)\}$ as long as one understands this is the set of all $x$ such that $P(x)$...{\bf if there is such a set}.  This notation might be undefined for some sentences.

The use of the complement notation $A^c$ is harmless as long as one understands that there is a fixed ``universal set" $X$  one has in mind from context:  this is really $X - A$ for some set understood from context.  We cannot really have a complement of a set $A$ in the absolute sense, $A^c = \{x:x \not\in A\}$.  If we did, then $A \cup A^c$ would be the universal set $V$ such that $x \in V$ for any $x$ at all, and then the axiom of separation would give us the set $\{x \in V:x \not\in x\}$, which we already know cannot exist.

I am expecting that you are all familiar with the method of Venn diagrams for giving visual demonstrations of relatively simple statements in set theory.  I will do some examples in class on Tuesday.

The definition of the Cartesian product $$A \times B = \{(a,b):a \in A \wedge b \in B\}$$ presupposes that you know what an ordered pair is.  You might not even notice this (the authors do not make heavy weather of it) but it is worth noting that this concept must either be postulated or explained in terms of sets.

The basic property of ordered pairs is that $(a,b) = (c,d)$ implies $a=c$ and $b=d$.  Defining $(a,b)$ as $\{a,b\}$ would not work, because if $a$ and $b$ are distinct, we want $(a,b)$ and $(b,a)$ to be distinct, and $\{a,b\}$ is the same set as $\{b,a\}$.

In fact, it is possible to define $(a,b)$ as $\{\{a\},\{a,b\}\}$.  To justify this requires work we will not do:  one has to prove that $\{\{a\},\{a,b\}\}=\{\{c\},\{c,d\}\}$ implies $a=b$ and $c=d$, which is a bit tricky.  Other definitions of the ordered pair as a set construction are also possible.

Notice that if $a \in A$ and $b \in b$ it follows that $(a,b)$ as we have defined it is in ${\cal P}({\cal P}(A \cup B))$, which we will write more briefly as ${\cal P}^2(A \cup B)$.

This allows us to show that $A \times B$ is provided by our axioms:

$$A \times B = \{(a,b):a \in A \wedge b \in B\} $$ $$ = \{k \in {\cal P}^2(A \cup B):(\exists a,b \in A \cup B:a \in A \wedge b \in B \wedge k = (a,b))\}.$$

This depends on $(a,b)$ being defined in the particular way given:  this determines the bounding set we use.  Other definitions might involve different bounding sets, but in any case no additional axioms are required to get Cartesian products.  Since I have given axioms for set theory, I should show the flag about this!  You are not responsible for the details of this definition, but that doesn't mean it might not be good for you to read it.

\section{Homework 6}

\begin{enumerate}

\item Do project 5.3.  

\item Do project 5.12.  

\item Do project 5.16.  

In projects 5.12 and 5.16, if an equation is false, give a counterexample using specific finite sets for $A,B,C$; if an equation is true, give a Venn diagram illustration but also write a proof using the strategies outlined above.  I'll do some similar examples on Tuesday to illustrate these instructions.  

\item Give a Venn diagram illustration of the identity $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$:  a formal proof is not expected.  

\item Write the recursive definitions requested in project 5.17:  statement and/or proof of versions of the de Morgan laws will carry extra credit (the proofs would be induction proofs).  

continued on next page!

\item Do project 5.21.  

\item  Prove the statement I gave in class:  for any sets $A,B$, if $A \times B = B \times A,$ then $A=B$ or one of $A$ and $B$ is empty.  

I will prove both parts of Proposition 5.20 in class Tuesday to give you an idea how to approach the last two  problems.

\end{enumerate}

\newpage

\section{March 1 lecture:  Talking about functions using sets}

The concept we are attempting to clarify is $f:A \rightarrow B$, $f$ is a function from the set $A$ to the set $B$.

The book gives two separate definitions of this concept.  One of them is unsatisfactory because it uses vague terms;  the other uses set language cleverly to reduce the concept of functions entirely to that of sets, but the problem I point out that it does not express precisely the same notion!

\begin{description}

\item[First definition of a function:]  Let $A$ and $B$ be sets.  A function $f$ from $A$ to $B$ is determined by three ingredients:

\begin{enumerate}

\item  The set $A$, which is the domain of the function.

\item  The set $B$, which is the codomain of the function.

\item A  rule which determines for each element $x$ of $A$ a unique element $f(x)$ of $B$.

\end{enumerate}

\end{description}

A typical example from your earlier mathematical experience:  in calculus, we write $f(x)=x^2+1$ for the function which takes a (real) input and gives a (real) output $x^2+1$.   The only thing that is given explicitly in this rule is the rule, but the context tells us what the domain and codomain are.

In this example we can bring up a related concept and point out that it is not the same as one we have listed.  The {\em range\/} of the function $f(x)=x^2+1$  is the set of real numbers greater than or equal to 1.  This is not the same as the codomain, the expected type of output of the function.  This relates to the vexed issue we will talk about below.

The weakness of this definition is...what is a rule, precisely?

The second definition attempts to fix this.

\begin{description}

\item [Second definition of a function:]  A function from $A$ to $B$ is a subset of $A \times B$ with the property that for each $a \in A$ there is exactly one pair $(a,b)$ such that $(a,b) \in f$.   For each $a \in A$, we define $f(a)$ as the unique $b$ such that $(a,b) \in f$.

\end{description}

This is a mathematically satisfactory definition of what a function is.  I do not dispute that.  In fact, it is my favorite definition of what a function is.  The problem in this as in many books is that it is not equivalent to the first definition, and the authors act as if it is.

The exact difficulty is that the set $f$ implementing a function from $A$ to $B$ exactly captures the {\em rule\/} component of a function in the first sense, and the set  of all first components of pairs in $f$ captures the {\em domain\/} component of the function in the first sense, but it is impossible to determine what the codomain is from the subset of $A \times B$.
The range of $f$, ${\tt rng}(f)$, can be defined as the set of all second components of pairs belonging to $f$, and then any set $B$ at all such that ${\tt rng}(f) \subseteq B$ might be the codomain.

On page 86 in the book, there is a definition:

\begin{description}

\item[Definition:]  A function $f:A \rightarrow B$ is {\em surjective\/} if for each $b \in B$ there exists $a \in A$ such that $f(a)=b$.

\end{description}

This is a commonly used and important mathematical concept.  But it doesn't make sense with the second definition of function, which is presumably the one the authors are using.

Let $f$ be the calculus function $\{(x,y) \in {\mathbb R} \times {\mathbb R}:y=x^2+1\}$.  This is clearly a function from
$\mathbb R$ to $\mathbb R$, and it is not surjective, because for example there is no $a$ such that $f(a)=0$.

Let $g$ be the calculus function $\{(x,y) \in {\mathbb R} \times [1,\infty):y=x^2+1\}$.  This is clearly a function from
$\mathbb R$ to $[1,\infty)$, and it is surjective, because for every $b\in [1,\infty)$, $f(\sqrt{b-1}) = b$.

But $f$ and $g$ are exactly the same set;  the same set cannot both have and not have a well-defined property.  And the definition of surjection is not a valid definition, because it depends on $B$, and, if using the second definition, given $f$ you cannot tell what $B$ is.

There are two ways to fix this.  Either we say that a function is a ``surjection to $B$" (or ``onto $B$", onto being another common word for this concept) so that the missing $B$ is supplied, or we define a function in such a way that the codomain is actually a component.  

A standard approach is to define a function $f$ as an ordered triple $(A,B,G)$ where $A$ is a set, $B$ is a set, and $G$ is a subset of $A \times B$ such that for each $a \in A$ there is exactly one pair in $G$ with first component $a$.  We then refer to $A$ as the domain of $f$, $B$ as the codomain of $f$, and $G$ as the graph of $f$.

I have to admit I am not quite sure how I will handle this.  If I have occasion to talk about onto maps (surjections) I can avoid difficulty by explicitly saying what the maps are onto.  I can show the flag by talking about subsets of $A \times B$ as graphs of functions $f: A \rightarrow B$ without necessarily being explicit about whether I think the graph is the same thing as the function.  Most of the time it will make no difference.  But precise and consistent definitions are part of what is taught in this course:  it causes me pain that this otherwise excellent book falls into this common and unnecessary sloppy error.

The book does very little with this concept in this section.  I asked you in class about counting the functions from $A$ to $B$ for small
finite sets $A$ and $B$, presenting them in arrow diagram form and indicating whether they are one-to-one (injections) or onto the intended codomain (surjections).  I give definitions to support homework questions along these lines:

\begin{description}

\item [one-to-one:] A function $f$ from $A$ to $B$ is one to one or an injection just in case for any $x,y \in A$, if $f(x)=f(y)$ then $x=y$ (or, equivalently, if $x \neq y$, it follows that $f(x) \neq f(y)$).

\item[onto (B):]  A function $f:A \rightarrow B$ is onto ($B$) or a surjection (to $B$) if for each $b \in B$ there exists $a \in A$ such that $f(a)=b$.

\end{description}

\newpage

\section{March 3 lecture:  Talking about relations using sets;  equivalence relations and partitions}

I'm going to give a more general definition of what a relation is than the book does.

\begin{description}

\item[Definition of relation?:]  A relation $R$ from $A$ to $B$ is a subset of $A \times B$.  We write $a \, R\, b$ for
$(a,b) \in R$.

\item[Definition of relation?:]  A relation $R$ is a triple $(A,B,G)$ such that $A$ is a set, $B$ is a set, and $G \subseteq A \times B$.  We call $A$ the domain of $R$, $B$ the codomain of $R$, $G$ the graph of $R$, and write $x \, R \, y$ for
$(x,y) \in G$.

\end{description}


This is different from the definition in the book in two ways.  I am more general in allowing different domain and codomain.  A relation from $A$ to $A$ I'll call a relation on $A$ just as the book does.  The tension between forms of the definition exists because the domain $A$ (and codomain $B$ if it is distinct) cannot actually be deduced from the graph of the relation.

In this section the only kind of relation we are discussing is an equivalence relation.  An equivalence relation is a relation $E$ from $A$ to $A$ such that

\begin{enumerate}
\item $E$ is reflexive:  for every $a \in A$, $a \, E \, a$

\item $E$ is symmetric:  for every $a,b \in A$ if $a \, E \, b$ then $b \, E \, a$.

\item $E$ is transitive:  for every $a, b, c \in A$, if $a \, E\, b$ and $b \, E \, c$ then $a \, E \, c$


\end{enumerate}

The problem I have with the author's way of presenting the definition is that whether a relation on $A$ is reflexive depends on what $A$ is:  the relation $\{(1,1),(2,2),(3,3)\}$ is reflexive (and an equivalence relation) on $\{1,2,3\}$
but not on $\{1,2,3,4\}$.  And for a general relation on $A$, one cannot tell what the domain of the relation is from its graph:  there might be objects in the intended domain which do not have the relation to anything, as in this example.

There are two ways to fix this:  one can either use the second definition of what a relation is, which adds the domain as a component of the function, or one can always say ``equivalence relation on $A$'' and identify relations with their graphs:  then $\{(1,1),(2,2),(3,3)\}$ is an equivalence relation on $\{1,2,3\}$ but not an equivalence relation on 
$\{1,2,3,4\}$.

Most of the time the issue simply won't arise.  We will see what style we settle on.

An equivalence relation $E$ on $A$ generally expresses the idea that the objects related by it are the same in some respect.  The relation on the members of the class of having the same birthday is an equivalence relation, for example.

An equivalence relation $E$ on $A$ divides $A$ into sets which we call ``equivalence classes":

\begin{description}

\item[Definition:]  If $a \in A$ and $E$ is an equivalence class on $A$, we define $[a]_E$ (which may be written just $[a]$ if we know what relation we are talking about) as $\{x \in A:x \, E\, a\}$.

\end{description}

So the equivalence classes in a section of Math XXX under the relation of having the same letter grade on Test I would be no more than five sets, for each student the set of students with the same letter grade as theirs.

We give an extended account of defining a relation, verifying that it is an equivalence relation, and describing its equivalence classes, which is closely related to the application of these ideas which will follow.

\begin{description}

\item[Definition:]  We define the relation $E$ on the integers:  $x \, E \, y$ iff $x-y$ is divisible by 3.

\item[Theorem:]  $E$ is an equivalence relation on the set of integers.

\item[Comment:]  A proof of such a statement follows a natural strategy:  we need to show that the relaiton is reflexive symmetric and transitive, and the definition of each of these properties suggests a logical setup.

\item[Proof:]  First we show that $E$ is reflexive.  What we need to show is that for any integer $x$, $x-x$ is divisible by 3.  Let $x$ be chosen arbitrarily.  $x-x=0$ and 0 is divisible by 3, finishing this part.

Now we show that $E$ is symmetric.  Let $x,y$ be arbitrarily chosen integers.  Assume that $3|(x-y)$  Our goal is to show that $3|(y-x)$.  Pause:  do you see that this is a strategy to prove that $E$ is symmetric?

Since $3|(x-y)$ there is an integer $k$ such that $3k=x-y$.  It follows that $y-x = -(x-y) = -3k = 3(-k)$, so $y-x$ is divisible by 3.

Now we show that $E$ is transitive.  Let $x,y,z$ be arbitrarily chosen integers.  Assume $3|(x-y)$ and $3|(y-z)$.
Our goal is to show $3|(x-z)$:  this will establish that $E$ is transitive.

Because $3|(x-y)$ and $3|(y-z)$ we have integers $k,l$ such that $3k=x-y$ and $3l=y-z$.  Then $x-z = (x-y) + (y-z) = 3k + 3l = 3(k+l)$, so $x-z = 3(k+l)$ is divisible by 3.

This completes the proof that $E$ is an equivalence relation.

\end{description}

Having shown that $E$ is an equivalence relation, we might ask what is the same about two numbers that stand in this relation.  In short, they have the same remainder on division by 3.  The equivalence classes are $[0] = \{\ldots,0,3,6,9,\ldots\}$,   $[1] = \{\ldots,1,4,7,10,\ldots\}$, $[2] = \{\ldots,2,5,8,11,\ldots\}$  there are no other equivalence classes, though of course every integer has an equivalence class:  $[5]=[2]$ for example.

Some theorems about equivalence classes which I proved in class.  In everything that follows, $E$ is an equivalence class on $A$, and $[a]$ is shorthand for $[a]_E$.

\begin{description}

\item[Theorem:]  $a \in [a]$

\item[Proof:]  $a \, E \,a$, because $E$ is reflexive, so $a \in \{x\in A:x \, E \, a\}=[a]$.  Notice that this shows that every equivalence class is nonempty.

\item[Theorem:]   $a \, E \, b \leftrightarrow [a] = [b]$

\item [Proof:]  Let $a,b \in A$ be chosen arbitrarily.

To prove that $[a]=[b]$ implies $a \,E\, b$, assume $[a]=[b]$.  Observe that $a \in [a]$ (previous theorem) so $a \in [b]$ (hypothesis) so $a \in \{x \in A:x \, E\, b\}$ (definition of $[b]$) so $a \,E\,b$.

To prove that $a\,E\,b$ implies $[a]=[b]$ we assume $a \, E\,b$ then argue that $[a]=[b]$.  For this we need our strategy for proving that two sets are equal.  

Assume $x \in [a]$:  we need to show that $x \in [b]$.  Since $x \in [a] = \{y \in A:y\, E \, a\}$, we have $x \, E\, a$.  We also have $a \, E \, b$ so by transitivity we have $x \, E \, b$, so we have $x \in \{y \in A:y \, E\, b\} = [b]$.  

Now assume $x \in [b]$ and show that this implies $x \in [a]$:  $x \in [b]$ implies $x \, E \, b$, so $b \, E \, x$ (symmetry) and we have $a \, E \, b$ and so $a \, E \, x$ by transitivity, and so $x \, E \, a$ by symmetry, and so $x \in [a]$.

This completes the proof.  Part of the purpose of the exercise is to recognize both that this is a proof, and that at every step I did what the situation naturally called for (this was not pulled out of the air).

\item[Theorem:]  For any $a,b \in A$, either $[a]=[b]$ or $[a] \cap [b] = \emptyset$ (any two equivalence classes are either identical or disjoint).

\item[Proof:]  If $[a] \cap [b] = \emptyset$  is true, we are done.  So let's suppose that $[a] \cap [b] \neq  \emptyset$ and show that $[a]=[b]$ must follow.

Because we have assumed that $[a] \cap [b] \neq  \emptyset$ we can postulate an element $x$ of $[a] \cap [b]$
(this is a useful general principle:  if we suppose a set nonempty, we can conjure up an element of it).

$x \in [a] \cap [b]$ implies $x \in [a]$ and $x \in [b]$ (definition of intersection)

thus $x \,E \,a$ and $x \, E \, b$ (def of equivalence classes)

thus $a \, E \,x$ and $x \, E \, b$ (symmetry)

thus $a \, E \, b$ (transitivity)

thus $[a] = [b]$ (previous theorem)

and we are done.

\item[Definition:]  Let $A$ be a set.  We say that $P \subseteq {\cal P}(A)$ (a collection of subsets of $A$) is a partition of $A$ just in case

\begin{enumerate}

\item each set $B \in P$ is nonempty

\item  For any $B,C \in P$, either $B=C$ or $B \cup C=\emptyset$

\item  For any $a \in A$, there is $B \in P$ such that $a \in B$

\end{enumerate}

\item[Claim 1:]  We have already shown that for any equivalence relation $E$ on $A$, the set $\{[a]_E:a \in A\}$ is a partition of $A$.  I will ask you to give the (very short) justification that this is the case, using theorems proved above.

\item[Claim 2:]  If $P$ is a partition of $A$, define $x \equiv_P y$ as $(\exists B \in P:x \in B \wedge y \in B)$.  Prove that $\equiv_P$ is an equivalence relation on $A$.  I outlined this in intuitive terms during lecture.

\end{description}

The two claims establish an exact correspondence between equivalence relations on $A$ and partitions of $A$.


\newpage
\section{Homework 7}

\begin{enumerate}

\item  How many functions are there from $\{a,b,c\}$ to $\{1,2\}$?  List all of them.  You may use arrow diagrams or explicit lists of ordered pairs.  Which of them are one-to-one?  Which of them are onto $\{1,2\}$?

\item  How many functions are there from $\{a,b\}$ to $\{1,2,3\}$?  List all of them.  You may use arrow diagrams or explicit lists of ordered pairs.  Which of them are one-to-one?  Which of them are onto $\{1,2,3\}$?

\item  How many functions are there from $\{1,2,3\}$ to $\{1,2,3\}$ which are onto $\{1,2,3\}$?  List all of them.  You may use arrow diagrams or explicit lists of ordered pairs.  Which of them are one-to-one?

\item Verify claim 1.  The verification of each part of the proof that the set of equivalence classes under $E$ is a partition is straight from one of our theorems.

\item Verify claim 2.

\item Do all parts of project 6.7. For each part indicate whether the relation has each of the three properties defining an equivalence relation (reflexive, symmetric, transitive).

\item  We define a relation on ${\bf N} \times {\bf N}$ (the set of pairs of positive integers).  $(x,y) SD (z,w)$ is defined as holding iff $x+w=y+z$.

Prove that $SD$ is an equivalence relation.  What is the same about $(x,y)$ and $(z,w)$ if they stand in this relation?

\end{enumerate}

\newpage

\section{3/8/2022:  the Division Algorithm and the Euclidean Algorithm}

I talked about computational things to do with the integers (mostly with the positive natural numbers).

First, I talked about the familiar operation of division:  as it is taught in elementary school, with whole number quotients and remainders.

\begin{description}

\item[Division Algorithm:]  Let $a$ be an integer and $b$ be a positive integer.  Then there are uniquely determined integers $q$ and $r$ such that $a=bq+r$ and $0 \leq r <b$.

\item[Proof:]   We prove this for nonnegative $a$ by induction on $a$.

Fix $b>0$.

Basis step:  let $a=0$.

$q=0$ and $r=0$ will work:  $0=a=b0 + 0$.  

And the only way that $0=a=bq+r$ can be true with
$0 \leq r < b$ is if $q=0$ and $r=0$:  we would have $bq = -r$ and since $b$ is positive, if $q$ were not zero we would have either $bq\geq b$ or $bq\leq -b$, and $-b < -r < b$ makes this impossible.  Of course, if $0=a=b0+r$, we have $r=0$.

Induction step:  Fix a nonnegative integer $k$.  Suppose that there are unique $q',r'$ with $0 \leq r' <b$ such that
$k=bq'+r'$.  We want to show that there are unique $q,r$ such that $k+1 = bq+r$ and $0 \leq r <b$.

If $r'<b-1$, then let $q=q'$, $r=r'+1$ and we have the desired result.  $qb+r = q'b+r'+1 = k + 1$, and 
$0<1 \leq r'+1 = r<b$.

If $r'=b-1$, then let $q=q'+1$, $r=0$ and we have the desired result.   $qb+r = (q'+1)b +0 = q'b+b = q'b+r'+1 = k+1$ and $0 \leq 0 = r <b$ is obvious.

Now if $qb+r = k+1$ and $1 \leq r <b$, we have $qb+(r-1) = k$ and still $0 \leq r-1<b$, so by ind hyp $q=q'$ and $r-1=r'$, so $q=q', r=r'+1$ when $r'<b$.

If $qb+r = k+1$ and $r=0$, then $(q-1)b+(b-1)=k$ and we have $0 \leq b-1 <b$, so by ind hyp
$q-1 = q'$ and $b-1=r'$, so $q=q'+1$ and $r=0$ when $r'=b-1$.

In both cases, we have verified uniqueness of $q$ and $r$.


\end{description}

This proof differs from what I did in class in proving uniqueness at the same time as existence.

Now the proof of existence and uniqueness of $q$ and $r$ when $a<0$:  if $a<0$ then $-a>0$, so there are unique
$q'$ and $0 \leq r'<b$ such that $-a = bq' + r'$.  If $r'=0$ we get $-a=b(-q') + r' = b(-q')$ and we can
set $q = -q'$ and $r=r'=0$.  If $r'>0$ we get $-a = b(-q'-1)+(b-r')$.  Notice that $0 \leq b-r' <b$ in this case.
So we set $q=-q'-1$ and $r=b-r'$.  

For uniqueness, if $-a=bq+r$ with $r=0$, we have $a=b(-q) + 0$ so in fact $-q=q'$ and we have the same
values as above.  If $-a=bq+r$ with $r>0$ then $a=b(-q) + (-r) = b(-q-1) +b-r$ and we must have $-q-1=q'$
and $b-r=r'$ by the uniqueness of $q'$ and $r'$ from which $q= -q'-1$ and $r=b-r'$ as above.

\begin{description}

\item[Definition:]  For $a$ an integer and $b$ a positive integer, we define $a {\tt div} b$ and $a {\tt mod} b$
as the unique $q$ and $r$ such that $a=bq+r$ and $0 \leq r <b$.

\item[Note about calculation:]  Note that (for positive $a$) you can compute $a {\tt div} b$ by dividing $a$ by $b$ and throwing out what occurs after the decimal point.  Then $a{\tt mod}b = a - b(a{\tt div} b)$.  This is how to compute remainders with a limited calculator.

\end{description}

Now we discuss the greatest common divisor (gcd) a concept for which the author gives a definition which seems to have little to do with the words in the phrase ``greatest common divisor".

If $a,b$ are integers, not both equal to 0, the author defines ${\tt gcd}(a,b)$ as the smallest element of the set
$\{ax+by: x \in {\mathbb Z} \wedge y \in {\mathbb Z} \wedge ax+by>0\}$.  It is straightforward to show that this
set is nonempty if one of $a,b$ is not zero.

The natural definition goes as follows:  a common divisor of $a,b$ is an integer $d$ such that $d|a$ and $d|b$.
Notice that if $a \neq 0$, any divisor of $a$ is less than $|a|$:  this means that there is an upper bound ${\tt max}(|a|,|b|)$ on common divisors of $a,b$ if $a,b$ are not both zero, and this in turn means that there is a largest common divisor of $a,b$, because any nonempty set of integers with an upper bound has a largest element.  and this is what we mean by ${\tt gcd}(a,b)$.

That these two definitions are equivalent is a substantial theorem.

That a nonempty set of integers which has an upper bound has a largest element is worth proving.  Suppose $A$ is a nonempty set and $b$ is an upper bound for $A$:  this means $(\forall x \in A:x \leq b)$.  Notice that this implies
$(\forall x \in A:x \leq b+1)$:  $b+1$ is then a strict upper bound for $A$.  Now consider the set $S = \{(b+1)-x:x \in A\}$.  $S$ is a nonempty set of positive integers and so has a smallest element $m$.  To make a long story short,
$m=(b+1)-g$ for some $g \in A$, by the definition of $S$, and this $g$ must be the largest element of $A$.

If $a,b$ are not both zero, the set of common divisors of $a$ and $b$ is nonempty, because it contains 1, and it
is bounded above by ${\tt max}(|a|,|b|)$, so it has a largest element, and this we call ${\tt gcd}(a,b)$.

Now we state and justify two facts about the gcd, using the natural definition.

${\tt gcd}(a,0)= |a|$.  This is obvious.  We will usually be working with positive integers, for which
${\tt gcd}(a,0) = a$.

${\tt gcd}(a,b) = {\tt gcd}(b,a{\tt mod}b)$.  This requires a supporting argument.  We prove this by showing
that $\{d \in {\mathbb Z}:d|a \wedge d|b\}$ and $\{d \in {\mathbb Z}:d|b \wedge d|a{\tt mod}b\}$ are the same set, so of course they have the same largest elements, and this proves the result.

If $d|a$ and $d|b$ then $d|b$ (of course) and $d|a {\tt mod}b$ because $a{\tt mod}b = a = b(a{\tt div} b)$:  anything which goes into both $a$ and $b$ will go into a number of the form $a-qb$ where $q$ is an integer.

If $d|b$ and $d|a{\tt mod}b$ then $d|a$ because $a = b(a{\tt div} b)+a{\tt mod}b$ and anything going into both $b$ and $a{\tt mod}b$ will go into both terms of this sum.  Of course $d|b$.

So we have shown that these two sets are equal, and so that their maxima, the two gcds, are equal.

This gives a procedure for evaluating ${\tt gcd}(a,b)$:  define a sequence by $r_1 = a; r_2 = b; r_{k+2}= r_k{\tt mod}r_{k+1}$ for each $k$, or $r_{k+2}$ is undefined if $r_{k+1}=0$.  Notice that if $r_{k+2}$ exists, it must be smaller than $r_{k+1}$, because $c{\tt mod}d$ is always less than $d$.  Since the sequence always gets smaller (after $r_2$ at least) it must eventually end in an $r_n=0$ by the well-ordering principle.  The second fact about gcd's tells us that
${\tt gcd}(r_k,r_{k+1}) = {\tt gcd}(r_1,r_2) = {\tt gcd}(a,b)$ for every $k$.  So ${\tt gcd}(r_{n-1},r_n) = r_{n-1} = {\tt gcd}(a,b)$:  the last nonzero term of this sequence will be the gcd.

This procedure is called the {\em Euclidean algorithm\/}.

A stronger version of the Euclidean algorithm can be used to show that for any $a,b$, not both zero,
there are integers $x,y$ such that $ax+by = {\tt gcd}(a,b)$.

What we do is show how to compute $x_i$ and $y_i$ such that $ax_i = by_i = r_i$ for each $i$:  then
$a_{n-1}$ and $b_{n-1}$ are what we are looking for.

Define $x_1$ as 1 and $y_1$ as 0.  Notice that $ax_1 +by_1=a=r_1$.

Define $x_2$ as 0 and $y_2$ as 1.  Notice that $ax_2 + by_2 = b = r_2$.

Now define $x_{k+2} = x_k - x_{k+1}(r_k{\tt div}r_{k+1})$ and $y_{k+2} = y_k - y_{k+1}(r_k{\tt div}r_{k+1})$.

Assume as inductive hypothesis that $ax_i + by_i = r_i$ for $i<k+2$.

$ax_{k+2} + by_{k+2} = ax_k - ax_{k+1}(r_k{\tt div}r_{k+1}) +by_k - by_{k+1}(r_k{\tt div}r_{k+1}) =
(ax_k + by_k) - (ax_{k+1}+by_{k+1})(r_k{\tt div}r_{k+1}) = $  [by ind hyp] $r_k - r_{k+1}(r_k{\tt div}r_{k+1})  = r_k{\tt mod}r_{k+1} = r_{k+2}$.

So since, if $n$ is the last term of the $r$ sequence, $r_n=0$, we have $r_{n-1} = {\tt gcd}(a,b)$, it follows
that $ax_{n-1}+by_{n-1}={\tt gcd}(a,b)$.

This computational procedure is called the extended Euclidean algorithm.  It is an exact description of the tables I draw:  the first three columns have at row $i$ the numbers $r_i, x_i, y_i$ (and I write $r_{i-2}{\tt div}r_{i-1}$ in the third column at and after row 3).

This in turn allows us to justify the assertion that ${\tt gcd}(a,b)$ is the smallest positive number of the form $ax+by$ with $x,y$ integers.  We have just shown that ${\tt gcd}(a,b)$ is a positive number of this form.  Now, since it goes intq $a$ and goes into $b$, it must go into every number $ax+by$.  No positive number less than ${\tt gcd}(a,b)$ can have ${\tt gcd}(a,b)$ as a divisor, so in fact no positive number less than ${\tt gcd}(a,b)$ can be of the form $ax+by$.

\section{3/10/2022:  Modular arithmetic, elementary definitions and results}

We developed basic definitions and theorems of modular arithmetic, which provide applications of both the abstract machinery of equivalence relations and the division algorithm (and will give applications of the extended Euclidean algorithm, as we will soon see).

\begin{description}

\item[Definition:]  Fix an integer $n>1$.  For integers $x,y$, we define $x \equiv_n y$ (also written $x \equiv y ({\tt mod}\,n)$) as
holding iff $n|(x-y)$.  We say in English, $x$ is congruent to $y$ mod $n$.

\item[Theorem:]  $\equiv_n$ is an equivalence relation.

\item[Proof:]  We first prove that $\equiv_n$ is reflexive.  $x \equiv_n x$ is true iff $n|(x-x)$, which is true because $n|0$ is true.

Then we prove that $\equiv_n$ is symmetric.  Suppose that $x \equiv_n y$.  Our goal is to prove $y \equiv_n x$.
Since $x \equiv_n y$, so $n|(x-y)$, there is an integer $k$ such that $x-y = kn$,  Now this implies that $y-x = (-k)n$ and so is divisible by $n$, and so $y \equiv_n x$.

Then we prove that $\equiv_n$ is transitive.  Suppose that $x \equiv_n y$ and $y \equiv_n z$.  Our goal is to show
$x \equiv_n z$.  Since $x \equiv_n y$ there is an integer $k$ such that $x-y=kn$. Since $y \equiv_n z$ there is an integer $l$ such that $y-z=ln$.   It follows that $x-z = (x-y)+(y-z)$ [this is the clever bit] $= kn + ln = (k+l)n$ is divisible by $n$, so $y \equiv_n z$.

\end{description}

An equivalence relation is to be thought of as saying that objects in its domain standing in this relation are the same in some way.  We show what is the same about $x,y$ if $x \equiv_n y$:

\begin{description}

\item[Theorem:]  $x \equiv_n y$ if and only if $x \,{\tt mod}\,n = y\,{\tt mod}\,n$.

\item[Proof:]

The proof has two parts, like the proof of any if and only if statement.

First, suppose that $x \equiv_n y$.  Our goal is to show that $x \,{\tt mod}\,n = y\,{\tt mod}\,n$.

By the division algorithm, $x = (x{\tt div}n)n + x{\tt mod}n$ and $y = (y{\tt div}n)n + y{\tt mod}n$.  So $x-y = 
(x{\tt div}n-y{\tt div}n)(n) + (x{\tt mod}n-y{\tt mod}n)$.  This is divisible by $n$ (and so $x \equiv_n y$) exactly if 
$(x{\tt mod}n-y{\tt mod}n)$ is divisible by $n$.  The difference of two nonnegative integers less than $n$ can only be divisible by $n$ if they are equal (try proving this!) and so if $x \equiv_n y$ we must have $(x{\tt mod}n-y{\tt mod}n)=0$ and so $x \,{\tt mod}\,n = y\,{\tt mod}\,n$.

Second, suppose that $x \,{\tt mod}\,n = y\,{\tt mod}\,n$.  Our goal is to show that $x \equiv_n y$.

We have $x = (x{\tt div}n)n + x{\tt mod}n$ and $y = (y{\tt div}n)n + y{\tt mod}n$  Thus, as above, $$x-y = 
(x{\tt div}n-y{\tt div}n)(n) + (x{\tt mod}n-y{\tt mod}n)$$  $= (x{\tt div}n-y{\tt div}n)(n)$ (because $x \,{\tt mod}\,n = y\,{\tt mod}\,n$) and this is divisible by $n$, so $x \equiv_n y$.
\end{description}

Equivalence relations determine a partition of their domain into equivalence classes.  In the case of $\equiv_n$, we partition the integers into $n$ equivalence classes, one for each remainder mod $n$ (classes $[0],[1],\ldots,[n-2],[n-1]$).  We could write these classes $[k]_{\equiv_5}$, but we can tell what the intended equivalence relation is from context.

For $n=5$ for example we have

$[0] = \{\ldots,-10,-5,0,5,10\ldots\}$

$[1] = \{\ldots,-9,-4,1,6,11\ldots\}$

$[2] = \{\ldots,-8,-3,2,7,12\ldots\}$

$[3] = \{\ldots,-7,-2,3,8,13\ldots\}$

$[4] = \{\ldots,-6,-1,4,9,14\ldots\}$

The class $[k]$ exists for any integer $k$, but it will be the same as $[k{\tt mod}5]$, one of these four classes.
For example, [19] is another name for [4].

The objects of the mathematical system called mod $n$ arithmetic, or ${\mathbb Z}_n$, are the equivalence classes
under $\equiv_n$.  What makes this an interesting mathematical system is that addition and multiplication have natural definitions on this system, which is the content of the following

\begin{description}

\item[Theorem:]  Suppose $x \equiv_n x'$ and $y \equiv_n y'$.  Then $x+y \equiv_n x'+y'$ and $xy \equiv_n x'y'$.

\item[Proof:]  Because $x \equiv_n x'$ and $y \equiv_n y'$ we have $x'-x$ and $y'-y$ divisible by $n$, so for some integers $k,l$ we have $x'=x+kn$ and $y'=y+ln$.

Now $(x'+y') - (x+y) = (x+kn+y+ln)-(x+y) = kn+ln$ which is divisible by $n$.  so $x+y \equiv_n x'+y'$.

Also, $x'y' - xy = (x+kn)(y+ln) - xy = xln + ykn+kln^2$, which is divisible by $n$, so $xy\equiv_n x'y'$.

The careful reader might notice that we are freely using the fact that the equivalence relation is symmetric.

\item[Definition:]  For sets of integers $A,B$, we define  $A+B = \{a +b:a \in A \wedge b \in B\}$ and $AB = \{ab:a \in A \wedge b \in B\}$.  We only use this definition in the special case where the sets are equivalence classes mod a fixed $n$ (at least, that is the only way we use it here:  it is not an unusual definition).

\item[Theorem:]  Fix $n>1$ and let $[x], [y]$ be equivalence classes mod $n$.  Then $[x]+[y] = [x+y] = [(x+y){\tt mod}n]$ and   $[x][y] \subseteq [xy] = [(xy){\tt mod}n]$.

\item[Proof:]  Any element of $[x]+[y]$ is of the form $x'+y'$ where $x' \in [x]$ and $y' \in [y]$ so $x' \equiv_n x$ and $y'\equiv_n y$, and by the theorem above this means that every element of $[x]+[y]$ is congruent mod $n$ to $x+y$ and belongs to $[x+y]$.  Any element of $[x+y]$ is congruent mod $n$ to $x+y$, so is of the form $(x+y)+kn$,
so is of the form $x+(y+kn)$, the sum of a number congruent mod $n$ to $x$ and a number congruent mod $n$ to $y$, and so an element of $[x]+[y]$, so we have shown that $[x+y]=[x]+[y]$.

Any element of $[x][y]$ is of the form $x'y'$ where $x' \in [x]$ and $y' \in [y]$ so $x' \equiv_n x$ and $y'\equiv_n y$, and by the theorem above this means that every element of $[x][y]$ is congruent mod $n$ to $xy$ and belongs to $[xy]$.  But we can't show in general that any element of $[xy]$ can be expressed as the product of an element of $[x]$ and an element of $y$:  $[xy]$ will have prime numbers in it for example which cannot typically be expressed as such a product.  (I said this wrong in class:  this is a correction).

\item[Definition:]  We define $[x]+[y]$ as $[x+y]$ and (re!)define $[x][y]$ as $[xy]$.  In the case of each operation,
if I take $x'$ from $[x]$ and $y'$ from $[y]$, all results of applying the operation to $x'$ and $y'$ will end up in the same equivalence class, so this definition works.

Notice that subtraction $[x]-[y]$ can also be defined as $[x-y]$.  We will have an notion of division in modular arithmetic, but it will be defined quite differently.

\end{description}

We give an example of the computation of addition and multiplication tables for a modulus, namely, $n=5$.

$$\begin{array}{c|ccccc}

+ & 0  & 1 & 2 & 3 & 4 \\ \hline
0 & 0  & 1 & 2 & 3 & 4\\
1 & 1 & 2 & 3 & 4 & 0\\
2  & 2 & 3 & 4& 0  & 1\\
3  & 3 & 4& 0  & 1 & 2\\
4  &4& 0  & 1 & 2 & 3 \\
\end{array}$$

$$\begin{array}{c|ccccc}

* & 0  & 1 & 2 & 3 & 4 \\ \hline
0 & 0  & 0 & 0 & 0 & 0\\
1 & 0 &1 & 2 & 3 & 4 \\
2  & 0 & 2 & 4& 1  & 3\\
3  & 0 & 3& 1  & 4 & 2\\
4  &0& 4  & 3 & 2 & 1 \\
\end{array}$$

Notice that each number mod 5 has an additive inverse:  the additive inverse of 0 is 0, the additive inverse of 1 is 4, the additive inverse of 2 is 3, the additive inverse of 3 is 2, the additive inverse of 4 is 1.  In a general modulus, the additive inverse of $x$ a nonzero remainder on division by $n$ will be $n-x$:  obviously $n-x \equiv_n -x$ and for $x$ a nonzero  remainder, $n-x$ is also a nonzero remainder.  This is not surprising:  the integers mod $n$ are derived from the integers and inherit the additive inverse property from the system of integers.

More surprisingly, each nonzero remainder has a {\em multiplicative\/} inverse.  We merely note this here for mod 5:  this will turn out to be true in general for moduli which are prime.  1 has multiplicative inverse 1, 2 has multiplicative inverse 3 ($2\cdot 3 = 1$), 3 has multiplicative inverse 2, 4 has itself as its multiplicative inverse.

It is easy-ish to see that systems of modular arithmetic have the properties of commutativity and associativity of each operation, distributivity, identities for both operations and the additive inverse property.

We show an example of the failure of the multiplicative cancellation property of the integers:

In mod 6 arithmetic, $(3)(2) \equiv_6 0$ and $(0)(2) \equiv_6 0$, with $2 \not\equiv_6 0$, but $3 \not\equiv_6 0$.

In the integers, if $ac= bc$ and $c \neq 0$ then we must have $a=b$.  This does not work in mod 6 arithmetic.

Even more striking is the fact that $(3)(2) \equiv_6 0$ illustrates that the zero factorization property of the integers fails in mod 6 arithmetic.

We will have more to say about this in the next lecture.

\section{Homework 9 for the Fall 2023 class, edited from Homework 8 for the 2022 class.}

Writing on 4/7/2023, this assignment is due 4/13/2023.

\begin{enumerate}

%\item (old business, I forgot to put this in an earlier assignment)   Do project 4.30 and project 4.31:  these are induction proofs involving the Fibonacci numbers, and I do want you to be able to do these kinds of problems.  You might notice that there was a proof of this kind in this week's notes!

\item  Evaluate each of the following expressions if it is possible to do so, and explain why it is not possible if it is not.

$200{\tt div}3, 200 {\tt mod}3,  (-200){\tt div}3,  (-200) {\tt mod}3,  200{\tt div}(-3),  200 {\tt mod}(-3)$

\item  Find $x$ and $y$ such that $137x + 111y = {\tt gcd}(137,111)$, using the extended Euclidean algorithm.  Your paper should show work following my table format.  I strongly suggest that you do this by hand, and only after that check using the spreadsheet.  You will need to be able to carry out this procedure efficiently on the next exam, several times, using a nongraphing calculator, so you should practice.

\item Do the same thing as in the previous problem but for larger numbers:  Find $x$ and $y$ such that $54321x + 12345y = {\tt gcd}(54321,12345)$

\item  Construct the addition and multiplication tables for mod 11 arithmetic, and make a table of multiplicative inverses of the nonzero remainders in mod 11 arithmetic, using your multiplication table.

\item  Find the square roots of 1 in mod 11 arithmetic.  These should be exactly what you expect, if you think carefully.  Now find the square roots of 1 in mod 8 arithmetic.  List them all.  Do you find something disturbing about this?

\item  This question is about square roots of $-1$.  Notice that $n-1 \equiv_n -1$:  in mod $n$ arithmetic, when we talk about $-1$, the additive inverse of 1,  we are actually talking about the remainder $n-1$.

This question is an exploration.  In each system of modular arithmetic for $n=2$ to 20, see if there is a square root of $-1$.  Just to help you see what is meant, there is one in mod 5 arithmetic, since $2^2 = 4 \equiv_5 -1$.  Notice that 3 is also a square root of $-1$ in mod 5 arithmetic.

If you can see a pattern to which moduli have square roots of -1, I will be impressed.

\end{enumerate}

\section{March 16:   Division and Exponentiation in Modular Arithmetic}

In this lecture, we talked about more mathematical operations in modular arithmetic.  I noted above that in mod 5 (for example) every nonzero number has a multiplicative inverse, so in fact it is possible to {\em divide\/}.  I give a general discussion of the situations under which division is possible in modular arithmetic.

First we prove a couple of theorems using the EEA (extended Euclidean algorithm) which you might think would normally be proved using facts about prime factorization;  in fact, it is quite the reverse:  the proof of the basic results about primes actually depend on EEA...

\begin{description}

\item[Theorem (Euclid's Lemma):]  If $p$ is a prime and $p|ab$, then either $p|a$ or $p|b$.

\item[Observation:]  It is not true for general natural numbers $n$ that $n|ab$ implies that either $n|a$ or $n|b$:
for example, $6|24 = 3 \cdot 8$, but $6 \not\in 3$ and $6 \not\in 8$.

\item [Proof of Euclid's Lemma:]  If $p|a$, we are done.  So suppose $p \not|a$ and we will show that $p|b$ has to be true.

Since $p \not| a$ and $p$ is prime, ${\tt gcd}(a,p) = 1$ (the only positive divisors of $p$ are 1 and $p$, so these are the only possible values of the gcd, and the value $p$ is ruled about because $p \not|a$).

By the EEA, there are integers $x$ and $y$ such that $ax+py = 1$.

Now $b = 1b = (ax+py)b = abx + byp$.  $abx$ is divisible by $ab$ which is divisible by $p$.  $byp$ is divisible by $p$.
So $b = abx + byp$ is divisible by $p$, which completes the proof.

\item[Theorem:]  If ${\tt gcd}(m,n) = 1$ and $n|mx$ then $n|x$.  This is a more general result which has Euclid's Lemma as a special case.

\item[Proof:]  Since ${\tt gcd}(m,n)=1$, we have integers $u$ and $v$ such that $mu+nv=1$.  So $x = 1x = (mu+nv)x = mxu + nvx$.  $mxu$ is divisible by $mx$ which is divisible by $n$.  $nvx$ is divisible by $n$.  Thus $x = mxu + nvx$ is divisible by $n$.

\item[Theorem (multiplicative inverses in modular arithmetic):]  Let $n>1$.  Suppose ${\tt gcd}(m,n)=1$.  Then there is an $a$ such that $am \equiv_n 1$.  Moreover, if $am \equiv_n 1$ and $a'm \equiv_n 1$ then $a \equiv_n a'$:  each equivalence class mod $n$ whose members are relatively prime to $n$ has a unique multiplicative inverse in our system of modular arithmetic.

\item[Proof:]   Because ${\tt gcd}(m,n) = 1$, there are integers $a, b$ such that $ma+nb=1$.  It follows that $ma \equiv_n 1$.

Now suppose that $ma \equiv_n 1$ and $ma' \equiv_n 1$.  It follows that $ma = 1 +kn$ for some integer $n$
and $ma' = 1+k'n$ for some integer $k'$, from which it follows that $ma-ma' = kn-k'n$ is divisible by $n$.

So we have $n|m(a-a')$ and by the previous theorem, since ${\tt gcd}(m,n) = 1$, it follows that $n|(a-a')$ so
$a \equiv_n a'$.

\item[Definition:]  We define $m^{-1} \,{\tt mod}\, n$ as the unique remainder mod $n$ such that $mn \equiv_n 1$, if there is one (this exists if and only if ${\tt gcd}(m,n)=1$).

\item[Remarks:]  Please note that the proofs of each of these three theorems are examinable:  they could appear on Test II.

The proof shows us not only that each $m$ has a unique multiplicative inverse, but also shows how to compute it (using the EEA).

\item[Example 1:]  Compute $12^{-1} {\tt mod} 137$


$$\begin{array}{c|c|c|c}

r & x & y & q \\ \hline

137 & 1 & 0 & \\

12 & 0 & 1 \\

5 & 1 & -11 & 11 \\

2 & -2 & 23 & 2 \\

1 & 5 & -57 & 2 

\end{array}$$


From this calculation, we see that $(5)(137) + (-57)(12) = 1$, so $(-57)(12) \equiv_{137} 1$.  $-57$ is not a remainder mod 137;  add 137 to get the value 80 of  $12^{-1} {\tt mod} 137$.

Check:  (80)(12) = 960 = 1 +7(137) $\equiv_{137} 1$.

\item[Example 2:]  Solve $5x \equiv_{123} 12$.

Observe that 5 is relatively prime to 123, so it has a multiplicative inverse mod 123.  Multiplying both sides of the equation by $5^{-1}{\tt mod}123$ will give the result, and this is actually a familiar mathematical procedure, though in an unfamiliar system:  to solve $ax=b$. divide both sides bt $a$.

$$\begin{array}{c|c|c|c}

r & x & y & q \\ \hline

123 & 1 & 0 & \\

5 & 0 & 1 \\

3 & 1 & -24 & 24 \\

2 &  -1 & 25 & 1 \\

1  & 2 & -49 & 1 \\



\end{array}$$

This calculation shows that $(2)(123) + (-49)(5) = 1$, so $(-49)(5) \equiv_{123} (74)(5) \equiv_{123} 1$.

Thus if $5x\equiv_{123} 12$ we have $x \equiv_{123} (74)(5x) \equiv_{123} (74)(12) = 888 \equiv_{123} 888-7(123) = 27$.

We check:  $27\cdot 5 = 135 = 123+12 \equiv_{123} 12$.


\end{description}

Now we look at exponentation (computation of $a^b {\tt mod} n$).  Since exponentiation is repeated multiplication, we do have that $(a^b {\tt mod} n \equiv_n (a {\tt mod} n)^b {\tt mod} n$.  Simple examples show that we cannot reduce the exponent $b$ mod $n$ in the same way we can reduce the base $a$.

For example, $2^2 {\tt mod} 10 = 4 \equiv_{10} 4$, but $2^{12} = 4096 \equiv_{10} 6$, so though we have $2 \equiv_{10} 12$ we do not have $2^2 \equiv 2^{12}$.

Nonetheless, we can effectively compute powers in modular arithmetic with very large exponents.  We give two examples.

\begin{description}

\item[Example 1:]  Compute $17^{256} {\tt mod} 100$.

One thing we are doing which makes this example easier is using mod 100:  all we have to do to find the remainder mod 100 of a positive integer is take the last two digits.

The other thing which makes this a relatively easy problem is that the exponent is a power of 2, which gives a hint of our approach to computing powers with general exponents in modular arithmetic.

$17^2 = 289 \equiv_{100} 89$

$17^4 \equiv_{100} 89^2 = 7921 \equiv_{100} 21$

$17^8 \equiv_{100} 21^2 = 441 \equiv_{100} 41$

$17^{16} \equiv_{100} 41^2 = 1681 \equiv_{100} 81$

$17^{32} \equiv_{100} 81^2 = 6561 \equiv_{100} 61$

$17^{64} \equiv_{100} 61^2 = 3721 \equiv_{100} 21$

$17^{128} \equiv_{100} 21^2 = 441 \equiv_{100} 41$

$17^{256} \equiv_{100} 41^2 = 1681 \equiv_{100} 81$

\end{description}

$17^{256}$ is a number with probably at least 500 digits, and it would be an enormous calculation to determine it and then extract its last two digits.  We have done much less work here, by this method
of repeated squaring.


An exponent which is a power of 2 makes an exceptionally easy example but we can actually do this for any exponent, with one more computational observation.


\begin{description}

\item[Example 2:]  Compute $17^{355}{\tt mod}100$

In this example we stick with the easy modulus of 100 to minimize the work of computing remainders.  We start by setting up the powers we will compute, by
starting with the exponent 355 and successively dividing it by two, throwing away remainders, until 1 is reached.

$17^{355} = (17^{177})^2(17) \equiv_{100} 77^2(17) = 100793 \equiv_{100} 93$

$17^{177} = (17^{88})^2(17) \equiv_{100} 41^2(17) = 28577 \equiv_{100} 77$

$17^{88} = (17^{44})^2 \equiv_{100} 21^2 = 441 \equiv_{100} 41$

$17^{44} = (17^{22})^2 \equiv_{100} 89^2 = 7921 \equiv_{100} 21$

$17^{22} = (17^{11})^2 \equiv_{100} 33^2 = 1089 \equiv_{100} 89$

$17^{11} = (17^5)^2(17) \equiv_{100} 57^2(17) = 55233 \equiv_{100} 33$

$17^5 = (17^2)^2(17) \equiv_{100} 89^2(17) = 134657 \equiv_{100} 57$

$17^2  = 289 \equiv_{100} 89$

$17^1 = 17$



\end{description}

Notice that we handle odd exponents by squaring the previous exponent and adding one more factor equal to the base (in this case one more factor of 17).  This allows calculation
of any exponent.  The number of multiplications is proportional to the log base 2 of the exponent.  Notice that no number larger than the cube of the base is ever computed.

An example in full generality (we will see some in RSA computations in the next section)  is a bit harder than this because in the general case one has to compute remainders
on division by numbers less convenient than 100.
\subsection{The Chinese Remainder Theorem}

Next I lectured the Chinese Remainder Theorem.   (Spring 2023 students note that this subsection was stolen from notes for Math 189)

\begin{description}

\item[Theorem:]  If $m,n >1$ and ${\tt gcd}(m,n)=1$, each system of equations of the form

$x\equiv_m a$

$x \equiv_n b$

has a unique solution mod $mn$.

\item[Proof (and method of computation!):]  Because $x \equiv_m a$, there is an integer $k$ such that
$x = a+mk$.

So to find $x$ solve for $k$ the equation 

 $a+mk \equiv_n b$  [it is convenient here to let $m$ be the larger of the two moduli, but the procedure works either way]

which is equivalent to

$mk \equiv_n (b-a)$  [notice that $m$ and $b-a$ can be replaced with their remainders mod $n$ since this is a calculation in mod $n$ arithmetic]

Because ${\tt gcd}(m,n)=1$, there is a reciprocal $m^{-1} {\tt mod} n$ (which we know how to compute using the extended Euclidean algorithm)

Thus we get

$k \equiv_n (m^{-1} {\tt mod} n)(b-a)$

so for some integer $l$ we have $k=(m^{-1} {\tt mod} n)(b-a) + ln$

and we have $x = a + m(m^{-1} {\tt mod} n)(b-a) + lmn$, that is

$x \equiv_{mn} a + m(m^{-1} {\tt mod} n)(b-a)$.

Strictly speaking our reasoning shows that a solution $x$ must be of this form.  You can check directly that

$a + m(m^{-1} {\tt mod} n)(b-a) + lmn \equiv_m a$ [this is obvious, everything but the $a$ is a multiple of $m$]

and

$a +m(m^{-1}{\tt mod} n)(b-a) + lmn \equiv_n a + 1(b-a) + 0 = b$,

so in fact every number of this form is a solution and we have an exact description of the solutions.

I do observe that the formula here does provide a compact way to compute solutions to these problems, which I have seen students use successfully.  I will carry out the described computation step by step in examples, that being my particular practice.


\end{description}

I reproduce the example I did in class, which also illustrates that more than two equations of the form $x \equiv_m a$ for different moduli $m$ can be solved at the same time, as long as the moduli are pairwise relatively prime.

\begin{description}

\item[Example:]  23 pirates are dividing a stack of pieces of eight.

The pieces of eight are divided evenly under jealous eyes into 23 stacks...but there are four too few to make the piles even.

A scuffle ensues, cutlasses are drawn, and four pirates are interred in unmarked graves.

But now the pile of pieces of eight has to be divided again among the 19 remaining pirates.

There are two too few for an even division, and in the ensuing fracas two more pirates are despatched.

Now the 17 remaining pirates divide the loot and it comes out even!  They sail off in various directions in search of more mischief...

How many pieces of eight are there?

[Disclaimer:  no pirates were actually harmed in the creation of this word problem]

\item[Solution:]

We have the following equations ($x$ being the unknown number of pieces of eight):

$x \equiv_{23} -4 \equiv_{23} 19$

$x \equiv_{19} -2 \equiv_{19} 17$

$x \equiv_{17} 0$

We solve the first pair of equations.

$x \equiv_{23} \equiv_{23} 19$

$x \equiv_{19} \equiv_{19} 17$

We begin by setting $x=19+23k$ using the first equation.

We then want to solve for $k$ in

$19+23k\equiv_{19} 17$

which is equivalent to 

$23k \equiv_{19}-2$

which is equivalent to 

$4k \equiv_{19} 17$ [replace 23 and $-2$ with their remainders mod 19]

so we need to compute $4^{-1}{\tt mod} 19$ (which is not hard to guess but we will compute it)

$\begin{array}{cccc}

19 & 1 & 0 & \\
4 & 0 & 1 &\\
3 & 1 & -4 & 4\\
1 & -1 & 5 & 1 \\
\end{array}$

\end{description}

so $4^{-1}{\tt mod}19 = 5$

so $k \equiv_{19} (17)(5)\equiv_{19} 9$

and $x \equiv_{437} 19+(23)(9) = 226$ [437 = $mn$ = (23)(19)]


and now we have two equations

$x \equiv_{437} 19+(23)(9) = 226$ 

$x \equiv_{17} = 0$

We set $x=226+437k$

to get a new equation

$226+437k\equiv_{17}0$

The remainder of 226 on division by 17 is 5

The remainder of 437 on division by 17 is 12

so we have

$5+12k \equiv_{17}0$

$12k \equiv_{17} -5 \equiv_{17} 17-5 = 12$

so we don't have to do anything elaborate to find $k=1$.

Thus

$x \equiv_{(437)(17)} 226+437 = 663$

437 times 17 is 7429

so our solution is

$x \equiv_{7429} 663$.

If I want to force an exact answer, I could ask what the smallest possible number of coins was.  Or I could give extra information such as...we know they didnt have more than eight thousand coins.

\section{Homework 10 for Spring 2023 class, edited from original homework 9}

A few little exercises:

\begin{enumerate}

\item  Compute $108^{-1} {\tt mod} 211$

\item  Solve $108x \equiv_{211}  37$  (hint:  you can use the previous part!)

\item  Describe all solutions (if there are any) of $10x \equiv_{30} 20$.  Notice that 10 is not relatively prime to 30:  you need to test lots of values.  One solution is obvious:  there are more.

Now describe all solutions of $10x \equiv_{30} 15$.

This question is just a hint that things are more complicated when one solves linear equations in modular arithmetic and a multiplicative inverse is not available.

\item  Compute $13^{211} {\tt mod} 100$

\item  Compute $13^{211} {\tt mod} 137$  (this involves more work with remainders)

\item  (Chinese Remainder Theorem) 

Solve the system of equations

$x \equiv_{125} 111$

$x \equiv_{137} 14$

for $x$.  Give the smallest nonnegative solution and the general form of all solutions.

\item  A fresh pirate word problem (there is a very similar problem in the notes):

35 pirates are dividing a stack of pieces of eight.

The pieces of eight are divided evenly under jealous eyes into 35 stacks...but there are two  too few to make the piles even.

A scuffle ensues, cutlasses are drawn, and two  pirates are interred in unmarked graves.

But now the pile of pieces of eight has to be divided again among the 33  remaining pirates.

There are four too few for an even division, and in the ensuing fracas four more pirates are despatched.

Now the 29 remaining pirates divide the loot and it comes out even!  They sail off in various directions in search of more mischief...

How many pieces of eight are there?  It wasn't that good  a year for pirates...you can be confident that the smallest possible answer is correct.

[Disclaimer:  no pirates were actually harmed in the creation of this word problem]

\begin{comment}

\item  Use Fermat's Little Theorem to compute $5^{21355} {\tt mod} 17$.

Use the lemma about computation of $\phi(pq)$ to compute $7^{1000} {\tt mod} 35$.  This calculation will end up with some easy repeated squaring, not nearly as much as with an exponent of 1000.

\item  Let $N = (13)(17) = 221$.  Leet $r=5$.   Verify that $r$ is an appropriate encryption exponent
and determine the decryption exponent $s$.

Encrypt the message 100 then decrypt the result.

\end{comment}


\end{enumerate}


\section{March 18:  Euler's Theorem about exponentiation in modular arithmetic, and the RSA encryption algorithm}

There were two topics in the March 18 lecture, Euler's Theorem on efficient evaluation of exponentials in modular arithmetic, and a description of the RSA cryptosystem, a practical application of the mathematics we have been discussing and, I think, a nice capstone to this unit before we switch to continuous mathematics after the break.

We begin with a definition of a concept you would see more about in classes in number theory or related subjects:

\begin{description}

\item[Definition (Euler phi function):]  For each positive integer $n$, we define $\phi(n)$ as the number of elements in the set $$\{m \in {\mathbb Z}^+:m \leq n \wedge {\tt gcd}(m,n) = 1\}.$$

Another way of putting this is that $\phi(n)=1$ and for each $n>1$, $\phi(n)$ is the number of remainders mod $n$ which are relatively prime to $n$, or equivalently, the number of equivalence classes mod $n$ whose elements are relatively prime to $n$.

Notice that for any prime $p$, $\phi(p) = p-1$.  $\phi(10)$, for example, is 4, the number of elements in the set $\{1,3,7,9\}$.

\item[Lemma:]  If $p$ and $q$ are primes, $\phi(pq) = (p-1)(q-1)$.

\item[Proof of Lemma:]  The remainders mod $pq$ which are {\em not\/} relatively prime to $pq$ are 0,
$q-1$ positive multiples of $p$, and $p-1$ positive multiples of $q$, so there are $pq-1-(p-1)-(q-1) = pq-p-q+1 = (p-1)(q-1)$ remainders mod $pq$ which are relatively prime to $pq$.

\item[Euler's Theorem:]  For each $n>1$, for each $a$ with ${\tt gcd}(a,n)=1$, $a^{\phi(n)} \equiv_n 1$.

\item[Proof of Euler's Theorem:]  Consider the product $\prod_{0<i<n \wedge {\tt gcd}(i,n)=1} ai$ of all the products of remainders mod $n$ which are relatively prime to $n$ with $a$.

This product is equal to $a^{\phi(n)}\prod_{0<i<n \wedge {\tt gcd}(i,n)=1} i$ by pulling out the factor of $a$ from each of the $\phi(n)$ factors $ai$ of the original product,

This product is also equivalent mod $n$ to  $\prod_{0<i<n \wedge {\tt gcd}(i,n)=1} ai{\tt mod}n$, obviously.

Less obviously, $\prod_{0<i<n \wedge {\tt gcd}(i,n)=1} ai{\tt mod}n = \prod_{0<i<n \wedge {\tt gcd}(i,n)=1} i$,
because these two products are in fact products of the same numbers in a different order.  The factors $ai{\tt mod}n$ in the first product are all different because we have $$ai \equiv_n aj \rightarrow (a^{-1}{\tt mod}n)ai \equiv_n (a^{-1}{\tt mod}n)aj \rightarrow i=j,$$ and of course each $ai$ is relatively prime to $n$ since both $a$ and each $i$ are so, from which it follows that both these products are the same, the product of all the remainders mod $n$ which are relatively prime to $n$.

We give an example.  If $n=10$ and $a=3$, $\prod_{0<i<10 \wedge {\tt gcd}(i,n)=1} 3i{\tt mod}10 = 3\cdot9\cdot1\cdot 7 = 1\cdot3\cdot7\cdot 9 =\prod_{0<i<10 \wedge {\tt gcd}(i,n)=1} i{\tt mod}10$.  The products are the same but in a different order.

It follows from the calculations above that $a^{\phi(n)}\prod_{0<i<n \wedge {\tt gcd}(i,n)=1} i \equiv_n \prod_{0<i<n \wedge {\tt gcd}(i,n)=1} i$, and since $\prod_{0<i<n \wedge {\tt gcd}(i,n)=1} i$ is relatively prime to $n$, it has a multiplicative inverse mod $n$, and we can multiply both sides of the congruence by this inverse to get $a^{\phi(n)} \equiv_n 1$..

\item[Corollary (Fermat's Little Theorem):]  If $p$ is a prime and $p \not| a$, then $a^{p-1} \equiv_p 1$.  This follows immediately from Euler's Theorem because $\phi(p) = p-1$ and $p \not| a \rightarrow {\tt gcd}(a,p)=1$.

\item[Observation:]  This implies that $a^b \equiv_n (a {\tt mod} n)^{b {\tt mod} \phi(n)}$, which allows more efficient calculation of exponentials, at least for small $n$ than is supported by the method of completed squaring.


\end{description}

Now we have all the setup we need to describe the RSA cryptosystem, our final topic in this unit of our class on number theory.  Though it might not be evident from anything we have said so far, this is applied math.  The number theory we have been talking about is the basis of modern public-key cryptography.  We are going to describe the oldest of the modern public key ciphers, proposed in 1978, which made number theory a branch of applied mathematics.

The distinguishing characteristic of public key cryptography is that the method of sending me a secure message is public, and the message can be transmitted over public channels.  The reason for this is that decryption of the message depends on secret information which I have and do not need to share with anyone, which so far as anyone knows is very difficult to determine from my public key.

The public key consists of two numbers, $N$ and $r$.  A message $M$ is scrambled to send to be by computing
$M' = M^r {\tt mod}N$ and transmitting it to me.  $N$ is typically a number of hundreds or thousands of digits.  The message $M$ should be a positive integer less than $N$:  a longer message is encrypted by breaking it into blocks.

Now to reveal my secrets...in very general terms which do not compromise the security of any particular RSA key.  I know that $N$ is the product of two (large) primes $p$ and $q$.  I do not share these with anyone.  I know (because I chose $r$, that $r$ is relatively prime to $\phi(N) = \phi(pq) = (p-1)(q-1)$ [using the Lemma above].

Now I have computed $s = r^{-1} {\tt mod} (p-1)(q-1)$.  I can decrypt $M'$ by computing $(M')^s {\tt mod} N$.

We verify that this works.  $(M')^s {\tt mod}N = (M^r {\tt mod} N)^s {\tt mod} N = (M^r)^s {\tt mod} N = M^{rs} {\tt mod} N = M^{rs \,{\tt mod}\, (p-1)(q-1)} {\tt mod} N$ by Euler's Theorem, which is $M {\tt mod} N$ because $rs \, {\tt mod}\, (p-1)(q-1)= 1$ because  $s = r^{-1} {\tt mod} (p-1)(q-1)$, and this is simply $M$ because $0<M<N$.


All of this hinges  on the apparent fact that factoring $N$ into $p$ and $q$ if $p$ and $q$ are large enough and chosen with only moderate care appears to be extremely difficult.

The usefulness of this method depends, though, on the ability to find primes $p$ and $q$.  There are efficient tests for whether a number of hundreds of digits is prime or not.  And primes are fairly frequent:  if you generate a random number $p_0$ with say 500 digits, you can expect that there will be a prime within ${\tt log}(P) \sim 1500$ of $p_0$, so just test odd numbers 1 by 1 until one is found to get a prime $p$.

What might make one a little nervous is that this primality test can be applied to our key component $N$ and will say unequivocally that $N$ is composite.  How can we tell that $N$ is composite without threatening to be able to find its factors $p$ and $q$, which might seem to be the only evidence that $N$ is composite?

We give a hint of how we can test a number for being prime (or composite) without finding its factors.  Suppose we
believe $m$ to be prime.  Fermat's little theorem tells us that if we choosa $a<m$ at random, and verify that
${\tt gcd}(a,m)=1$ (of course, if it is not, $m$ is not prime) and then compute $a^{m-1} {\tt mod} m$, we expect this to be 1.  If it is not, we have discovered that $m$ is composite.  The most common primality test in cryptography involves applying a test like this (but slightly more sophisticated) to randomly chosen numbers below $m$ until excellent confidence is obtained that $m$ is prime.  This method does not seem to tell us anything about the factors of $m$ if it reports that $m$ is composite.  I've described this primality test and proved its basic properties in Math 406 here, for undergraduates:  it is not that much more sophisticated than the test using Fermat's little theorem which I have described, but avoids some technical problems with this test.

Suppose we apply this test to our $N=pq$.  Choose $a$ randomly less than $pq$ (which will almost certainly be relatively prime to $N$).  Compute $a^{pq-1} {\tt mod} N$.  This is equivalent to $a^{(pq-1){\tt mod}(p-1)(q-1)} {\tt mod} N$, and $pq-1 {\tt mod} pq-p-q+1$ will be $p+q-2$, and $a^{p+q-2}$ is most unlikely to be congruent to 1 mod $N$.  But knowing the value of $a^{p+q-2}{\tt mod}N$ does not seem to give us any particular handle on what $p$ and $q$ are.

Now we finish up by giving an extended example of encryption and decryption of a message in RSA.  We will correct the one we got wrong at the end of the lecture on Thursday, to really wrap things up.  This is as I said an illustration of why one needs to do such calculations very carefully and check regularly.

We proposed in class to give a full account where $N = 247$, $r=5$, $M=42$.

First, we consider the public transmission of the message:  $M' = M^r {\tt mod} N = 42^5 {\tt mod} 247 = 74$,

Secretly, as no one could possibly guess, 247 = (13)(19):  $p=13$, $q=19$, and $\phi(N) = (12)(18) = 216$.

Notice that $r=5$ is relatively prime to 216.

Our decryption exponent $s$ will be $5^{-1}{\tt mod}216$.
$$\begin{array}{c|c|c|c}
216  & 1  & 0&\\

5  &    0 &  1&\\

1   &   1  & -43  &-43
\end{array}$$

The reciprocal of 5 mod 216 is 216-43 = 173.

Below is the calculation by repeated squaring showing that $74^{173} {\tt mod} 247 = 42$:   the decryption succeeds.

$$\begin{array}{cc}

173  & 120^2(74) mod 247 = 42 \\

86  & 100^2 mod 247 = 120 \\

43  & 144^2(74)  mod 247 = 100 \\

21  &  74^2(74) mod 247 = 144 \\

10   & 120^2 mod 247 = 74 \\

5   &   42^2(74)  mod 247 = 120 \\

2  &    74^2 mod 247 = 42 \\

1   &  74

\end{array}$$






\newpage

\section{Axiomatics of the reals}

In this section, we discuss the basic axioms for the system of real numbers $\mathbb R$.  This will actually not be quite a complete collection of axioms:  a sign of this is all these axioms will also hold for the system of rational numbers $\mathbb Q$.

What we say will follow what is said at the beginning of chapter 8 of the Art of Proof, and you can consult sections 8.1 and 8.2 for the same information.

As primitive notions we have the set $\mathbb R$ of real numbers, with operations of addition and multiplication.

\begin{description}

\item[commutative laws:]  For any $x,y \in \mathbb R$, $x+y=y+x$ and $xy = yx$.

\item[associative laws:]  For any $x,y,z \in \mathbb R$, $(x+y)+z=x+(y+z)$ and $(xy)z=x(yz)$.

\item[distributive law:]  For any $x,y,z \in \mathbb R$, $x(y+z) = xy+xz$.

\item[identity:]  There is a real number 0 such that $x+0=x$ for every $x\in \mathbb R$.  There is a real number 1 such that $1 \neq 0$ and $x1 =x$ for every $x\in \mathbb R$.

\item[inverses:]  For each $x \in \mathbb R$, there is a real number which we call $-x$ such that $x+ -x=0$ and if $x \neq 0$ there is a real number $x^{-1}$ such that $xx^{-1}=0$.

\end{description}

Note that the only difference between this and the axioms for $\mathbb Z$ in section 1.1 is that this axiom set includes existence of multiplicative inverses $x^{-1}$ and does not include the cancellation property for multiplication.  But

\begin{description}

\item[Theorem:]  If $x,y,z \in \mathbb R$, and $xy = xz$ and $x \neq 0$, then $y=z$.

\item[Proof:]  Assume $x,y,z \in \mathbb R$, and $xy = xz$ and $x \neq 0$.

It follows that $x^{-1}(xy) = x^{-1}(xz)$ and thus $y = 1y = (xx^{-1})y = x^{-1}(xy) = x^{-1}(xz) = (xx^{-1})z = 1z = z$

\end{description}

This means that every theorem proved using the axioms of 1.1 as a theorem about the integers is also a theorem about the real numbers (all the theorems in section 1.2).  The notion of divisibility doesn't make sense for the reals, because it is too general:  ``there is a $z$ such that $xz=y$"  will hold in the reals for all $x,y$ unless $x = 0$ and $y \neq 0$.

The operation of division $\frac xy$ can be defined as $xy^{-1}$ as long as $y \neq 0$.  This is similar to the introduction of subtraction in chapter 1.

The axioms of order for the reals are introduced in the same way they were for the integers.

We assume the existence of a special set $\mathbb R_{>0}$ of {\em positive reals\/}.  This is more frequently written ${\mathbb R}^+$

We provide the following axioms:

\begin{description}

\item[closure:]  If $x,y \in \mathbb R_{>0}$, then $x+y \in \mathbb R_{>0}$ and $xy \in \mathbb R_{>0}$.

\item[zero is not positive:]  $0 \in \mathbb R_{>0}$.

\item[trichotomy:]  For any nonzero $x \in \mathbb R$, $x \in {\mathbb R}_{>0}$  or $-x \in {\mathbb R}_{>0}$

\end{description}

We define $x<y$ as $y-x \in \mathbb R_{>0}$.  We define $x>y$ as $y<x$, $x \leq y$ as $x<y \vee x=y$ and
$x \geq y$ as $y \leq x$.

Notice once again that this is exactly the same as the axiomatization of the integers.

The only axiom which we have provided for the integers which does not hold for the reals is the well-ordering principle.  And anything we can prove about the integers using our axioms which does not use the well-ordering principle (or, equivalently, mathematical induction) is a theorem about the reals as well.

We note that {\em all\/} the axioms for the reals we have given so far hold for the system of rational numbers.  And this should tell us that something is missing.  We know from school that for every positive real number $x$ there is a positive real number $\sqrt x$ such that $(\sqrt x)^2 = x$.  We also know from school that there is no rational number $r$ such that $r^2 = 2$, a fact which is commonly expressed by saying ``$\sqrt 2$ is irrational".

We will introduce the final axiom for the real numbers later.

We can show that the analogue of the well-ordering principle is false in the reals.

\begin{description}

\item[Theorem:]  The set of positive reals $\mathbb R_{>0}$ has no smallest element (so it is not the case that a nonempty set of positive integers must have a smallest element).

\item[Proof:]  We recall that 1 is positive.  Either 1 or $-1$ is positive.  If $-1$ were positive then by closure $(-1)(-1) = 1$ would be positive and $1+(-1) = 0$ would be positive, a contradiction.  Thus 1 is positive.

It follows that $2 =1+1$ is positive by closure.  Now $2\cdot 2^{-1} = 1$ is positive.  If $-2^{-1}$ were positive it would follow that $2\cdot (-2^{-1}) = -1$ is positive, which is impossible.  S0 $2^{-1}$ is positive.

We have $0<1<2$, so $0<2^{-1}<1$ (multiply through by the positive $2^{-1}$.

Now for any positive $x$ we have $0<x2^{-1}<x$, so for any positive $x$ there is a smaller positive real $x2^{-1} = \frac x2$.

\end{description}

\newpage

\section{Properties of absolute value}

In this section we discuss some properties of order and the notion of absolute value.

We all know this:

\begin{description}

\item[Theorem:]  For any real number $x$, $x^2\geq 0$

\item[Proof:]  If $x=0$, then $x^2=0 \geq 0$.

If $x \neq 0$ there are two cases:  either $x$ is positive or $-x$ is positive.

If $x$ is positive, then $x^2$ is positive and so greater than zero by closure of the positive reals under multiplication.

If $-x$ is positive, than $(-x)^2 = x^2$ is positive and so greater than zero for the same reason.

\end{description}

Here is another useful theorem.

\begin{description}

\item[Theorem:]  Suppose $x,y >0$.  Then $x^{-1}, y^{-1}>0$, and $x < y \leftrightarrow x^{-1}>y^{-1}$.

\item[Proof:]  Suppose $x$ is positive and $-x^{-1}$ is positive.  It follows that $xx^{-1} + x(-x^{-1}) = 0$ is positive, a contradiction.

Now suppose $0<x<y$.  Multiply through by the positive $x^{-1}y^{-1}$ to get $0<y^{-1}<x^{-1}$, so $x^{-1}>y^{-1}$.

\item[Lemma (for the alternative proof of the Triangle Inequality below):]  Suppose $x$ and $y$ are positive
and $x^2\leq y^2$.  It follows that $x \leq y$.

\item[Proof:]  Suppose $x$ and $y$ are nonnegative
and $x^2\leq y^2$.  Suppose for the sake of a contradiction that $x>y$.  It follows that $x^2 - y^2 = (x-y)(x+y)$ with $x-y$ positive and $x+y$ positive (because $x$ and $y$ are not both zero).  So it follows that $x^2>y^2$, contradicting the assumption that $x^2 \leq y^2$.

\end{description}

I may add more theorems here if they come up in the discussion of limits in class.

Now we define the notion of absolute value.

\begin{description}

\item[Definition:]  If $x$ is a real number, we define $|x|$, the absolute value of $x$ as $x$ if $x \geq 0$, and as $-x$ if $x<0$.  Notice that $|x|$ is always positive if $x \neq 0$.

\item[Important Observation:]  It is important to notice that $|x-y|$ is the distance from $x$ to $y$ on the real number line.

\item[Theorem:]  $|x|^2 = x^2$.

\item[Theorem:]  $|xy| = |x||y|$.

\end{description}

It is NOT true that $|x+y| = |x|+|y|$.  Nor is it true that $|x-y| = |x|-|y|$.  These things can be true under special conditions.

If $x\geq 0$ and $y\geq 0$, then $x+y\geq 0$ and so $x=|x|, y=|y|,x+y=|x+y|$, so $|x+y| = x+y = |x|+|y|$.

If $x<0$ and $y<0$, then $x+y<0$ and we have $|x|=-x, |y|=-y, |x+y| = -(x+y)$, so $|x+y| = -(x+y) = (-x)+(-y) = |x|+|y|$.

If $x\geq 0$ and $y<0$ then $|x|=x$ and $|y| = -y$.  It follows that $x+y = |x|-|y|$, and $|x+y|= $ either
$|x|-|y|$ (if $|x|>|y|$) or $|y|-|x|$ (if $|y|>|x|$).  Notice that $|x|-|y|<|x|+|y|$ and $|y|-|x|\leq |y|+|x| = |x|+|y|$.
In either case we have $|x+y| \leq |x|+|y|$.

So in all four cases we have established

\begin{description}

\item[Theorem (Triangle Inequality):]  For any real numbers $x,y$, $|x+y| \leq |x|+|y|$.

\item[Another proof:]  Let $x,y$ be real numbers.

We observe that $|x+y|^2 = (x+y)^2 = x^2+2xy+y^2 \leq x^2+|2xy|+y^2 = |x|^2+2|x||y|+|y|^2 = (|x|+|y|)^2$.

$|x+y| \leq |x|+|y|$ follows by the Lemma given above.


\end{description}

But why is this called the Triangle Inequality?  The Triangle Inequality is a geometrical statement.  Let $d(A,B)$ represent the distance between points $A$ and $B$.  The Triangle Inequality is the assertion that $d(A,C) \leq d(A,B) + d(B,C)$.  Now for the real numbers, we can define $d(x,y) = |x-y|$, and then the assertion $d(x,-y) \leq d(x,0) + d(0,-y)$ is an instance of the Triangle Inequality and translates to $|x-(-y)| \leq |x-0|+|0-(-y)|$, which is exactly 
$|x+y| \leq |x|+|y|$.   Further the derivation $d(x,z) = |x-z| = |(x-y)+(y-z)| \leq |x-y| + |y-z| = d(x,y) + d(y,z)$ shows that the general form of the triangle inequality follows from the specific instance we give as our theorem, for distances on the real line.

\newpage

\section{The definition of limit}

Now we use the notion of distance in the real numbers to define the notion of limit which was basically presented to you as a primitive idea in calculus.

$\lim_{x \rightarrow a}f(x) = L$ says (phrasing it in English and very carefully to make the development work neatly)
that you can force $f(x)$ to be within any desired distance of $L$ (as close to $L$ as you like) by choosing $x$ close enough to $a$ (but different from $a$).

We rephrase this:  for any positive real number $\epsilon$, we can force $|f(x)-L|<\epsilon$ by by choosing $x$ close enough to $a$ (but different from $a$).

We turned the first degree of closeness into a number.

We rephrase again.  For any positive real number $\epsilon$, there is a positive real number $\delta$ such that $|f(x)-L|<\epsilon$ will be true if $|x-a|<\delta$ and $x \neq a$.

We turned the second degree of closeness into a number.  Of course the closeness of $x$ to $a$ that we need is determined by the closeness of $f(x)$ to $L$ that we demanded initially.

We clean things up a little, and bring everything into logical notation.

\begin{description}

\item[Definition:]  We define $\lim_{x \rightarrow a}f(x) = L$ as meaning

$$(\forall \epsilon>0:(\exists \delta >0:(\forall x:0<|x-a|<\delta \rightarrow |f(x)-L|<\epsilon)))$$

The symbol $\lim_{x \rightarrow a}f(x)$ is undefined if there is no $L$ which makes this statement true.

\end{description}

\newpage

\section{Limit examples and theorems}

When you learned calculus, you learned to use some theorems about limits, which in effect were axioms for you, since you were no more given proofs of these theorems than you were given a definition of the limit concept.

\begin{description}

\item[Theorems (basic properties of limits):]

\begin{description}

\item

\item[constants:]  $\lim_{x \rightarrow a}c=c$ for any constant $c$.

\item[identity function:]  $\lim_{x \rightarrow a} x = a$

\item[limits of sums:]  $$\lim_{x \rightarrow a}[f(x) + g(x)] = \lim_{x \rightarrow a}f(x) +\lim_{x \rightarrow a}g(x)$$ if the limits on the right side of the equals sign exist.

\item[limits of constant multiples:]   $$\lim_{x \rightarrow a}cf(x) = c\lim_{x \rightarrow a}f(x)$$ if the limit on the right hand side of the equation exists. 

\item[limits of differences:]  Using the two previous theorems, $$\lim_{x \rightarrow a}[f(x)-g(x)] = \lim_{x \rightarrow a}[f(x)+(-1)g(x)]$$ $$ = \lim_{x \rightarrow a}f(x) + \lim_{x \rightarrow a}[(-1)g(x)] $$ $$= \lim_{x \rightarrow a}f(x) + (-1)\lim_{x \rightarrow a}g(x) = \lim_{x \rightarrow a}f(x) - \lim_{x \rightarrow a}g(x),$$ if the two limits on the right exist.

\item[limits of products:]  $$\lim_{x \rightarrow a}[f(x) \cdot g(x)] = \lim_{x \rightarrow a}f(x) \cdot \lim_{x \rightarrow a}g(x)$$ if the limits on the right side of the equals sign exist.

\item[limits of quotients:]  $$\lim_{x \rightarrow a}\frac{f(x)}{ g(x)} = \frac{\lim_{x \rightarrow a}f(x)}{ \lim_{x \rightarrow a}g(x)}$$ if the limits on the right side of the equals sign exist and the denominator of the fraction on the right is not zero.

\end{description}


\end{description}

In your experience in calculus, these theorems are the basic tools for understanding what limits are computationally, aided perhaps by some intuition.

We will prove most or all of these theorems.

There are more theorems of the same description but these are enough to start with.

Now we do a series of proofs of example limit statements and of theorems from the list above.

\begin{description}

\item[Example:]  Prove that $\lim_{x\rightarrow 2}[ 3x+1] = 7$

The first step is to express what this statement means in full:  For each $\epsilon>0$, there is a $\delta >0$ such that for all $x$ if $0<|x-2|<\delta$ then $|(3x+1)-7|<\epsilon$.

The form of the statement suggests a strategy (which we will see is used in general in these proofs):

Let $\epsilon <0$ be chosen arbitrarily.

Let $\delta$ be $\ldots$

Let $x$ be chosen arbitarily.

Assume $0<|x-2|<\delta$

Goal:  $|(3x+1)-7|<\epsilon$

There is a gap in this development.  We have to say what fills in the dots (what $\delta$ actually is).  Notice that the choice of $\delta$ can depend on $\epsilon$, but not on $x$.

What we generally do is first do some ``scratch work" to figure out a $\delta$ that might work.  We do this by figuring backward from the desired
outcome $|f(x)-L|<\epsilon$ to the $0<|x-a|<\delta$ which will work.  Notice that this is a dangerous activity, because the proof has to go  in the other direction.

{\bf The scratch work:}

We want $|(3x+1)-7|<\epsilon$

This is equivalent to $|3x-6|<\epsilon$

and so to $3|x-2|<\epsilon$

and so to $|x-2|<\frac{\epsilon}3$

So we take away from the scratch work the idea that $\delta =\frac{\epsilon}3$

and write {\bf the official proof}:

Let $\epsilon <0$ be chosen arbitrarily.

Let $\delta = \frac{\epsilon}3$

Let $x$ be chosen arbitarily.

Assume $0<|x-2|<\delta=\frac{\epsilon}3$

Goal:  $|(3x+1)-7|<\epsilon$

Since $0<|x-2|<\delta=\frac{\epsilon}3$, we have $|x-2|<\frac{\epsilon}3$

Multiply both sides by 3 and we get $3|x-2| < \epsilon$

From this we have $|(3x+1)-7| = |3x-6| = 3|x-2| < \epsilon$

so we have $|(3x+1)-7| < \epsilon$, our goal.

\end{description}

We prove some actual theorems.  I have taken these from my class notes for a Math 314 course in 2014, but they are quite similar to what I did in class.

\begin{description}

\item[Theorem (limit of a constant):]  $$\lim_{x \rightarrow a} c=c$$

Notice that the two $c$'s mean different things.  The first one is the constant function whose value is $c$,
the second is the real number $c$.

Choose $\epsilon_0>0$.  Let $\delta_0=1$.  Choose $x$ arbitrarily such that $0<|x-a|<\delta_0=1$.
Our goal is to show that $|c-c|<\epsilon_0$.  But $|c-c|=0<\epsilon_0$ is definitely true!  In fact it simply
does't matter what $\delta_0$ we choose.

\item[Theorem (limit of the identity function):]  $$\lim_{x \rightarrow a} x=a$$

Choose $\epsilon_0>0$.  Let $\delta_0=\epsilon_0$.  Choose $x$ such that $0<|x-a|<\delta_0=\epsilon_0$.
Our goal is $|x-a|<\epsilon$, but this is included in our assumption, so we are done.

\item[Theorem (constant multiple property of limits):]  If $\lim_{x\rightarrow a} f(x)= L$, then $\lim_{x \rightarrow a} cf(x) = cL$.

Outline of start of proof:

If $c=0$ then limit of a constant handles this, so we may suppose $c \neq 0$.

Assume $\lim_{x\rightarrow a} f(x)= L$.

Choose $\epsilon_0>0$ arbitrarily.

Let $\delta_0 =\ldots$

Choose $x$ arbitrarily.

Assume $0<|x-a|<\delta_0$.

Goal:  $|cf(x)-cL|<\epsilon_0$.

Now we are at the scratch work phase.

$|cf(x)-cL|=|c||f(x)-L|<\epsilon_0$ will be enforced if $|f(x)-L|<\frac{\epsilon_0}{|c|}$.

Because we assumed $\lim_{x\rightarrow a} f(x)= L$, we know that there is a $\delta_0>0$ such that
for each $x$ with $0<|x-a|<\delta_0$, we have $|f(x)-L|<\frac{\epsilon_0}{|c|}$.

Let this be our choice of $\delta_0$ in the proof.

The proof resumes, choosing $\delta_0$ so that for each $x$ with $0<|x-a|<\delta_0$, we have $|f(x)-L|<\frac{\epsilon_0}{|c|}$.

Since we have assumed $0<|x-a|<\delta_0$ it follows that $|f(x)-L|<\frac{\epsilon_0}{|c|}$.  Thus
$|cf(x)-cL| = |c||f(x)-L| < |c|\frac{\epsilon_0}{|c|}=\epsilon_0$, completing the proof.

It is important to notice that we do not state this theorem ``$\lim_{x\rightarrow a}cf(x)=c\lim_{x\rightarrow a} f(x)$".
Suppose that $c=0$ and $\lim_{x\rightarrow a}f(x)$ is undefined.  In this case the left side of this equation would be 0
and the right side would be undefined.

\item[Theorem (addition property of limits):]  if $\lim_{x \rightarrow a} f(x)=L$ and $\lim_{x \rightarrow a}g(x)=M$ then
$$\lim_{x \rightarrow a} f(x)+g(x) = L+M.$$

Note first of all that $$\lim_{x \rightarrow a} f(x)+g(x) = \lim_{x \rightarrow a}f(x)+\lim_{x \rightarrow a}g(x)$$ does not say the same thing.  If $f(x)=\frac 1x$ and $g(x)=-\frac 1x$ then the left side is 0 and the right side is undefined.

The proof starts.  

Assume $\lim_{x \rightarrow a} f(x)=L$ and $\lim_{x \rightarrow a}g(x)=M$.

Choose $\epsilon_0>0$.

Let $\delta_0=\ldots$

Choose $x$ arbitrarily.

Assume $0<|x-a|<\delta_0$.

Goal:  $|(f(x)+g(x))-(L+M)|<\epsilon_0$

We pause for scratch work.

$|(f(x)+g(x))-(L+M)| = |(f(x)-L) + (g(x)-M)| \leq |f(x)-L| + |g(x)-M|$.  This will be less than $\epsilon_0$
if we make  $|f(x)-L|<\frac{\epsilon_0}2$ and $|g(x)-M|<\frac{\epsilon_0}2$  By limit assumptions, we can
choose $\delta_1$ so that if $0<|x-a|<\delta_1$, then $|f(x)-L|<\frac{\epsilon_0}2$  and choose $\delta_2$ so that if $0<|x-a|<\delta_2$, then $|g(x)-M|<\frac{\epsilon_0}2$.  The point is that the limit statements about $f$ and $g$
allow us to find $\delta$'s corresponding to any value of $\epsilon$, in this case a value half as large as the value being considered for the sum function.  Let $\delta_0 = \min(\delta_1,\delta_2)$.  Continue the proof.

Since we have assumed $0<|x-a|<\delta_0$, we also have $0<|x-a|<\delta_1$ and $0<|x-a|<\delta_2$, so
we have $|f(x)-L|<\frac{\epsilon_0}2$ and $|g(x)-M|<\frac{\epsilon_0}2$.  Thus $|(f(x)+g(x))-(L+M)| = |(f(x)-L)+(g(x)-M)| \newline \leq |f(x)-L| + |g(x)-M| < \frac{\epsilon_0}2 +\frac{\epsilon_0}2 = \epsilon_0$.


\end{description}

Here are some examples in the spirit of our last lecture (again, taken from 2014 notes).

\begin{description}

\item[Example 1:]  $$\lim_{x\rightarrow 3} x^2=9$$

We outline the proof:

Choose an $\epsilon_0>0$ arbitrarily.

Let $\delta_0 = \ldots$

Choose $x$ arbitrarily.

Assume that $0<|x-3|<\delta_0$

Goal:  $|x^2-9|<\epsilon_0$

The next phase is scratch work to figure out what $\delta_0$ should be.
r
We aim to make $|x^2-9|<\epsilon_0$.

$|x^2-9| = |x+3||x-3| < \epsilon_0$ will be true if $|x-3|<\frac{\epsilon_0}{|x+3|}$

We  cannot set $\delta_0=\frac{\epsilon_0}{|x+3|}$, because this expression depends on $x$.  What we need is
$|x-3|< \frac{\epsilon_0}{???}< \frac{\epsilon_0}{|x+3|}$ where ???, whatever it is, does not depend on $x$.
We need ??? to be {\em greater than\/} $|x+3|$ (so that its reciprocal will be smaller).  To get an upper bound on $|x+3|$,
we impose an upper bound on $x$:  the only way we have to do this is to make stipulations about $\delta_0$.
If we impose $\delta_0 \leq 1$, then we get $|x-3|<1$, which is equivalent to $2<x<4$.  We then get $5 <x+3<7$,
and since $x+3>5>0$ we have $|x+3|=x+3<7$.  7 is the desired upper bound.  So we get $|x-3|< \frac{\epsilon_0}{7}< \frac{\epsilon_0}{|x+3|}$ implies $|x^2-9|<\epsilon_0$ as long as we also stipulated $|x-3|<1$.  So a workable value
of $\delta_0$ is $\min(1,\frac{\epsilon_0}7)$.

Now we continue the proof, setting $\delta_0 = \min(1,\frac{\epsilon_0}7)$.  

Since $|x-3|<\delta_0$, we also have $|x-3|<1$ and $|x-3|<\frac{\epsilon_0}7$.  From this we deduce $5<|x+3|=x+3<7$ just as we did above in the scratch work.  Now $|x^2-9|=|x+3||x-3|<7|x-3|<7\frac{\epsilon_0}7=\epsilon_0$.

\item[Example 2:]  $\lim_{x \rightarrow 0}\frac{|x|}x$  does not exist.

We prove this by assuming that $\lim_{x \rightarrow 0}\frac{|x|}x=L$  for some $L$ and deducing a contradiction.

Since  $\lim_{x \rightarrow 0}\frac{|x|}x=L$, there is a $\delta_0$ such that for any $x$, if $0<|x-0|=|x|<\delta_0$
then $|\frac{|x|}x-L|<1$  (here setting $\epsilon=1$),  Now set $x=\frac{\delta_0}2$:  we get $|1-L|<1$,
and similarly if we set $x=-\frac{\delta_0}2$, we get $|-1-L|=|L+1|< 1$  So we get $2=|(1-L)+(1+L)| \leq |1-L|+|1+L|<1+1=2$, a contradiction.

(I did this one a bit differently in class in 2022 and I may try to give that version here later).

There are definitely details of motivation for you to track down and fill in here.

\end{description}

\newpage

\section{Homework 11 (and last) for the 2023 class:  edited from the Homework 10 for the 2022 class}

This set is due at the same time as the final take home exam, which will probably be distributed on Monday or Tuesday.  Its main function is to serve as practice, and examples you can ask me about in class or office hours to help you approach similar problems in the takehome.  It will probably simply be checked off in your gradebook.

\begin{enumerate}

\item  Use Fermat's Little Theorem to compute $5^{21355} {\tt mod} 17$.

Use the lemma about computation of $\phi(pq)$ to compute $22^{1000} {\tt mod} 35$.  This calculation will end up with some easy repeated squaring, not nearly as much as with an exponent of 1000.

\item  Let $N = (13)(17) = 221$.  Leet $r=5$.   Verify that $r$ is an appropriate encryption exponent
and determine the decryption exponent $s$.

Encrypt the message 100 then decrypt the result.

\item  Prove that for any two real numbers $a<b$ there is a real number $x$ such that $a<x<b$.  The familiar notion of the average might be useful.  But do prove your statements about the number $x$ that you find using the basic properties of order (you can assume familiar algebra of addition, subtraction, multiplication and even division, but be explicit about order).

\begin{comment}

\item  Give a proof that the open interval $(0,1) = \{x \in {\mathbb R}:0<x<1\}$ has no largest element.

Remember that we have proved (and used) the fact that a set of integers which is nonempty and bounded above has a largest element:  this is a result about the reals which is different from what can happen in the integers.

\end{comment}

\item  Prove that for any real numbers $x,y$, $|x| - |y| \leq |x-y|$.  Hint:  this follows from the Triangle Inequality and brief algebra.

\item Prove that $\lim_{x \rightarrow 3}2x+3 = 9$

\item  Prove that $\lim_{x \rightarrow 2}12-2x = 8$, from the definition of limit.

\item Prove that $\lim_{x \rightarrow 4}x^2 = 16$

\begin{comment}

\item  Prove that $\lim_{x \rightarrow 2}\frac{x^2-4}{x-2} = 4$ from the definition of limit.
Hint:  so far you have not used both parts of the hypothesis $0<|x-a|<\delta$;  in this proof you will.

\end{comment}

\end{enumerate}

\section{More limits}

\begin{description}


\item[Example 1:]  $\lim_{x \rightarrow 2}\frac1x = \frac12$

Start the proof as usual.

Let $\epsilon>0$ be chosen arbitrarily.

We do some scratch work to find $\delta$:  we want to make $|\frac 1x - \frac 12|<\epsilon$ by making $|x-2|$ small enough.

$|\frac 1x - \frac 12| = |\frac{2-x}{2x}| = \frac{|x-2|}{|2x|}$ and this will be less than $\epsilon$ just in case $|x-2|<|2x|\epsilon$.

$\delta$ cannot depend on $x$:  but we can get a condition which works if we can place a lower bound on $x$.

Assume $|x-2|<1$, or equivalently $1<x<3$, so we now want $|x-2|<2\cdot 1\epsilon < 2x\epsilon = |2x|\epsilon$

so we set $\delta = {\tt min}(1,2\epsilon)$.

Continue the proof.  Let $\delta={\tt min}(1,2\epsilon)$.

Choose $x$ arbitrarily.

Suppose that $0<|x-2|<\delta={\tt min}(1,2\epsilon)$.

It follows that $|x-2|<1$ so $1<x<3$.

Also $|x-2|<2\epsilon$.

Now $|\frac 1x - \frac 12| = \frac{|x-2|}{|2x|} = \frac{|x-2|}{2x}<\frac{|x-2|}{2}$ (because $x<1$) $<\frac{2\epsilon}2=\epsilon$.

We aren't going to develop the quotient rule for limits, but this is a first step in that process.

\item[Example 2:]  $\lim_{x \rightarrow a}x^2 = a^2$

This result is a generalization of proofs we have already done for particular values of $a$.

Choose $\epsilon>0$ arbitrarily.

We do some scratch work.  We want to get $|x^2-a^2|<\epsilon$ by making $|x-a|$ small enough.

$|x^2-a^2| = |x-a||x+a|$ which is less than $\epsilon$ just in case $|x-a| < \frac \epsilon{|x+a|}$.

We want a lower bound on $\frac \epsilon{|x+a|}$, for which we need an upper bound on $|x+a|$ not depending on $x$.

We use the triangle inequality cleverly:  $|x+a| = |(x-a)+2a| \leq 1+2|a|$ by the triangle inequality.

If we assume $|x-a|<1$, we then get $|x+a|  < 1+2|a|$

which means we want $|x-a| < \frac\epsilon{1+2|a|}< \frac \epsilon{|x+a|}$

And so we want $\delta = {\tt min}(1,\frac\epsilon{1+2|a|}).$

Now we resume the proof.

Let $\delta = {\tt min}(1,\frac\epsilon{1+2|a|}).$

Choose $x$ arbitrarily.

Suppose $0 < |x-a| <\delta = {\tt min}(1,\frac\epsilon{1+2|a|}).$

It follows that $|x-a|<1$, so $|x+a| = |(x-a)+2a| \leq |x-a|+|2a| < 1+2|a|$

Also. $|x-a|<\frac\epsilon{1+2|a|}$.

It follows that $$|x^2-a^2| = |x-a||x+a| < |x-a|(1+2|a|) $$ $$< \frac\epsilon{1+2|a|}(1+2|a|) = \epsilon.$$

\end{description}

I'm trying a perhaps easier approach to the product rule for limits.

\begin{description}

\item[Theorem:]  If $\lim_{x \rightarrow a}f(x) = L$, then $\lim_{x \rightarrow a}f(x)^2 = L^2$.

\item[Proof:]  Choose an arbitrary $\epsilon>0$.

We pause for scratch work.  We want to control the size of $|f(x)^2 - L^2|$ by controlling the size of $|x-a|$.

$|f(x)^2 - L^2| = |f(x)-L||f(x)+L|$.

For any $\epsilon_1$, we can choose a $\delta_1$ such that if $0<|x-a|<\delta_1$ it follows that $0<|f(x)-L|<\epsilon_1$.

In particular, we can choose a $\delta_2$ such that if $0<|x-a|<\delta_1$ it follows that $0<|f(x)-L|<1$.

$|f(x)+L| = |(f(x)-L)+2L| \leq |f(x)-L| + 2|L|$, so if $0<|x-a|<\delta_2$ we have $|f(x)+L| = |(f(x)-L)+2L| \leq |f(x)-L| + 2|L|\leq 1+2|L|$

Thus $|f(x)^2 - L^2| = |f(x)-L||f(x)+L| \leq |f(x)-L|(1+2|L|)$, so we can force $|f(x)^2 - L^2|<\epsilon$ by
forcing $|f(x)-L|< \frac{\epsilon}{1+2|L|}$.

Choose $\delta_3$ such that for any $x$, if $0<|x-a|<\delta_3$ we get $|f(x)-L|< \frac{\epsilon}{1+2|L|}$.

Set $\delta = {\tt min}(\delta_2,\delta_3)$.

We are ready to continue the proof.

Choose a $\delta_2>0$ such that if $0<|x-a|<\delta_1$ it follows that $0<|f(x)-L|<1$.

Choose $\delta_3$ such that for any $x$, if $0<|x-a|<\delta_3$ we get $|f(x)-L|< \frac{\epsilon}{1+2|L|}$.

Let $\delta={\tt min}(\delta_2,\delta_3)$.

Suppose that $0<|x-a|<\delta={\tt min}(\delta_2,\delta_3)$.

It follows that $0<|f(x)-L|<1$ so $|f(x)+L| = |(f(x)-L)+2L| \leq |f(x)-L| + 2|L|\leq 1+2|L|$.

It also follows that $|f(x)-L|< \frac{\epsilon}{1+2|L|}$

so $$|f(x)^2 - L^2| = |f(x)-L||f(x)+L| \leq |f(x)-L|(1+2|L|)$$ $$< \frac{\epsilon}{1+2|L|}(1+2|L|) = \epsilon.$$

\item[Theorem:]  If $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M$, then 
$\lim_{x \rightarrow a}f(x)g(x) = LM$.

\item[Proof:]  Suppose $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M$.

$\lim_{x \rightarrow a}f(x)g(x) = \lim_{x \rightarrow a}\frac 14[(f(x)+g(x))^2 - (f(x)-g(x))^2]$ if it exists.

We then apply theorems we have already proved.

Because $\lim_{x \rightarrow a}f(x) = L$ and $\lim_{x \rightarrow a}g(x) = M$, $\lim_{x \rightarrow a}[f(x)+g(x)]= L+M$ and $\lim_{x \rightarrow a}[f(x)-g(x)]= L-M$ (sum and difference rules of limits)

because $\lim_{x \rightarrow a}[f(x)+g(x)]= L+M$,$\lim_{x \rightarrow a}[f(x)+g(x)]^2= (L+M)^2$

because $\lim_{x \rightarrow a}[f(x)-g(x)]= L-M$,$\lim_{x \rightarrow a}[f(x)-g(x)]^2= (L-M)^2$

because $\lim_{x \rightarrow a}[f(x)+g(x)]^2= (L+M)^2$ and $\lim_{x \rightarrow a}[f(x)-g(x)]^2= (L-M)^2$,
$\lim_{x \rightarrow a}[(f(x)+g(x))^2 - (f(x)-g(x))^2]=(L+M)^2 - (L-M)^2$  (difference rule)

Because $\lim_{x \rightarrow a}[(f(x)+g(x))^2 - (f(x)-g(x))^2]=(L+M)^2 - (L-M)^2$, we have 

$$\lim_{x \rightarrow a}f(x)g(x) = \lim_{(x \rightarrow a}\frac 14[f(x)+g(x))^2 - (f(x)-g(x))^2] $$ $$= \frac14((L+M)^2 - (L-M)^2) = LM.$$(constant multiple rule and algebra).

\end{description}

The executive summary is that because of the algebraic identity $uv = \frac14((u+v)^2 - (u-v)^2)$ which reduces multiplication to addiction, subtraction, multiplication by a constant, and squaring, it is enough just to add a rule for squaring to our limit arsenal and then derive the rule for multiplication from the new squaring rule and previous rules.

\section{The difference between the reals and the rationals:  the least upper bound axiom}

We note, as we have already said, that every axiom we had for the integers also holds in the reals, except for the well-ordering principle, which we can show is false (using the additional property of existence of multiplicative inverses, which we have for the reals but not for the integers.

The system of rational numbers satisfies every axiom we have stated so far for the reals!

We will identify a statement you all ``know" to be true about the reals and which we can prove is false in the rationals.
This is the assertion that every positive real number has a square root.

\subsection{The square root of 2 is irrational}

This theorem goes about about as far in the history of mathematics in Europe as we have records.

Suppose on the contrary that for some integers $a,b$ we have $(\frac ab)^2 = 2$.

We can assume that the fraction is in simplest form:  were it not, we could divide top and bottom by ${\tt gcd}(a,b)$ and it would be in simplest form (the numerator and denominator would then have no common factors).

Then $a^2 = 2b^2$.

From this we can conclude that $a^2$ is even

from which we can further conclude that $a$ is even

Thus $a=2m$ for some integer $m$ and we have

$(2m)^2 = 2b^2$

so $4m^2 = 2b^2$

so $2m^2 = b^2$

so $b^2$ is even, so $b$ is even.

But this is a contradiction:  if $\frac ab$ is in simplest form, $a$ and $b$ cannot have the common factor 2.

So, there is something we believe in the background about the reals which we do not believe about the rations.  Presently we will see what it is (it is not just the assertion that there are square roots).

\subsection{The definition of the derivative, with an example, and its relation to one of your homework problems}

We remind you of an application of the limit notion we have already seen.

We can define $f'(a) = \lim_{x \rightarrow a}\frac{f(x)-f(a)}{x-a}$.

$\frac{f(x)-f(a)}{x-a}$ is the slope of the line segment from $(x,f(x))$ to $(a,f(a))$ (which you might remember is called a secant of the graph).  The derivative of $f$ at $a$ is then the limit as $x$ approaches $a$ of the slope of this secant.  That the limit exists suggests that the graph is smooth near $a$, and we can reasonably think of $f'(a)$ as the slope of the curve $y=f(x)$ at the point $(a,f(a))$.

We verify the familiar fact that if $f(x)=x^2$, $f'(x)=2x$.

$f'(a) = \lim_{x \rightarrow a}\frac{f(x)-f(a)}{x-a} = \lim_{x \rightarrow a}{x^2-a^2}{x-a} = \lim_{x \rightarrow a}\frac{(x-a)(x+a)}{x-a}=2a$

We pause here for just a moment to justify the last equation above.   Suppose $\epsilon>0$.  Let $\delta=\epsilon$.  Suppose $0<|x-a|<\delta$.  We want to show that
$|\frac{(x-a)(x+a)} - 2a| <\epsilon$.  {\bf Crucial magic step}:  because $0<|x-a|$, so $x\neq a$, we have $\frac{(x-a)(x+a)}=x+a$,
so $|\frac{(x-a)(x+a)} - 2a|=|x+a-2a|=|x-a|<\epsilon$.  This should also indicate to you how to do one of the problems in Homework 10.  In your calculus book, this might have been handled just by an offhand remark that $x \neq a$, so the fraction cancellation is legal.

So, we get an indication that we can cash out our careful analysis of the limit concept to go forward into calculus.

\subsection{definition of limit of a sequence?}  I discovered on peeking into the Art of Proof that I have gone down a different path in my discussion of limits.  I have been presenting an analysis of the concept ``limit of a function" which you learned about in Calc I.  The book actually discusses the concept ``limit of a sequence" from Calc II.

We briefly discuss the definition, which is similar in spirit to the definition of limit of a function.

$\lim_{i \rightarrow \infty}a_i = L$ means ``for each $\epsilon>0$ there is a natural number $N$ such that for any natural number $i$ if $i>N$, then $|a_i-L|<\epsilon$.

In this definition, $\delta$, measuring closeness of $x$ to the $a$ it is approaching, is replaced by $N$, measuring how large the index $i$ has to be to make $a_i$ as close as desired to $L$.

In general proofs of basic rules for calculation with limits of sequences are similar to those for limits of functions.

We prove that if $\lim_{i \rightarrow \infty}a_i=L$, it follows that $\lim_{i \rightarrow \infty}ca_i = cL$.

We choose $\epsilon>0$.

We choose $\delta$ so that for any $x$, if $0<|x-a|<\delta$, then  $|a_i-L|<\epsilon_{|c|+1}$ (this is a different trick to deal with the special case $c=0$, which we could also have used with the constant multiple rule for limits of functions).

Suppose $0<|x-a|<\delta$ just chosen.  Then $|ca_i -cL| = 0 <\epsilon$ if $c=0$, and otherwise $|ca_i - cL| = |c||a_i-L| < |c|\frac\epsilon{|c|+1} < |c|\frac\epsilon{|c|}=\epsilon$.

A hint about arguments from this definition, taken from experience:  when we want to have $0<|x-a|<\delta_1$ and $o<|x-a|<\delta_2$, we can collapse this into bounding $|x-a|$ by a single $\delta = {\tt min}(\delta_1,\delta_2)$.  But if we want $i>N_1$ and $i>N_2$ (which could happen in an analogous proof about limits of sequences) we need $i>{\tt max}(N_1,N_2)$, maximum instead of minimum.

\subsection{The Least Upper Bound Axiom, with definitions of terms}

Let $S$ be a set of real numbers.  We say that $b$ is an upper bound of $S$ just in case $(\forall s \in S:b \geq s)$.

We say that $B$ is a {\em least upper bound\/} of $S$ and write $B=\sup(S)$ just in case $B$ is an upper bound of $S$, and for every upper bound $b$ of $S$, $b \geq B$.

I invite you to think about why a set must have no more than one least upper bound. 

Some sets do not have least upper bounds.  The empty set does not have a least upper bound (can you explain why?)  The set of all real numbers or the set of all integers do not have least upper bounds because they have no upper bounds at all.

The axiom for the reals which we have been missing so far is the Completeness Axiom, or Least Upper Bound axiom:

\begin{description}

\item[Axiom:]  Any nonempty set of real numbers $S$ which has some upper bound has a least upper bound.

\end{description}

We invite you to write out the definitions for the concept of greatest lower bound of $S$, written $\inf(S)$, and prove that
a set with a lower bound has a greatest lower bound (Hint:  you might want to look at how we proved when discussing gcds that a set of integers with an upper bound has a largest element;  the logic is similar, using the set of additive inverses of elements of $S$).

If a set $S$ has a largest element ${\tt max}(S)$, then certainly for all $s \in S$, $s \leq {\tt max}(S)$, so ${\tt max}(S)$ is an upper bound of $S$, and for any $b < {\tt max}(S)$, $b$ cannot be an upper bound of $S$ because ${\tt max}(S)$ is an element of $S$, so if a set $S$ has a maximum element, the maximum element is also the least upper bound of $S$.

If a set $S$ has no largest element but is bounded above, and so has a least upper bound $\sup(S)$, we observe that for any $\epsilon>0$, $\sup(S)-\epsilon$ is not an upper bound of $S$, so there is an element $x$ of $S$ greater than $\sup(S)-\epsilon$ for which we have $x \in S$ and $\sup(S)-\epsilon<x<\sup(S)$.  In this case, the least upper bound is a kind of upper limit of the set, though it does not belong to it.

A familiar example is the open interval $(0,1)$, which has least upper bound 1 though 1 does not belong to the interval.

\subsection{Square roots of positive numbers exist, in the reals}

I did this as a demonstration in class, but I do not consider it examinable.  Please see the discussion in the Art of Proof, starting on page 103.

\newpage

\section{Homework 11 (and last)}

This is not homework for the 2023 class:  the previous set of exercises is your last homework.

\begin{enumerate}

\item Prove that $\lim_{x \rightarrow 3}\frac 1x = \frac 13$

\item Prove that $\lim_{i \rightarrow \infty} a_i + b_i = \lim_{i \rightarrow \infty} a_i + \lim_{i \rightarrow \infty}  b_i$, if the limits on the right exist.   You can probably find this by looking it up, but I strongly suggest using the addition rule for limits of functions as a model, and reading the last hint I give in the discussion of limits of sequences.

Being able to translate an argument which works in one context, and which you have above in the notes, into an argument which works in another context, is a useful skill and exactly the sort of thing we are looking for in this class.

\item  Give the definitions for lower bound of a set of reals and greatest lower bound of a set of reals.

Prove that a set $S$ of reals with a lower bound has a greatest lower bound.  Hint:  think about $-S = \{-s:s \in S\}$.

\item  What are the least upper bound and the greatest lower bound of $[1,2)$?  What are the least upper bound and greatest lower bound of the set $\{2-\frac1x:x \in \mathbb N\}$?  What are the least upper bound and greatest lower bound of the set
$\{3 + \frac{(-1)^i}{x^2}:x \in \mathbb N\}$?

\end{enumerate}

Ill be happy to do any of these on Thursday in class.  Some of them are possible test questions (the first and the last).  The other two are harder, but worth thinking about as final exercises in the unit we are doing.







\end{document}